{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "320e3402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import keras\n",
    "from tensorflow.keras.optimizers import Adam,SGD,RMSprop\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense,Flatten,Input,Dropout,Activation,BatchNormalization\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from tensorflow import keras\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pycm import *\n",
    "from keras_tuner.tuners import BayesianOptimization,Hyperband"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f3cc2",
   "metadata": {},
   "source": [
    "เทรน Model รอบแรก"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03dd8fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "\n",
    "    base_model  = InceptionV3(weights='imagenet',include_top=False,input_tensor = Input(shape = (224,224,3)))\n",
    "\n",
    "    x=base_model.output\n",
    "    x=Flatten()(x)\n",
    "    x=Dense(1024,activation=\"relu\")(x)\n",
    "    x=Dropout(0.3)(x)\n",
    "    x=Dense(1024)(x)\n",
    "    x=BatchNormalization()(x)\n",
    "    x=Activation(\"relu\")(x)\n",
    "    x=Dropout(0.3)(x)\n",
    "    x=Dense(512,activation=\"relu\")(x)\n",
    "    preds=Dense(400,activation='softmax')(x)\n",
    "\n",
    "    model=keras.Model(inputs=base_model.input,outputs=preds)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.compile(optimizer=\"adam\",loss='categorical_crossentropy',metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5638a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data_generator(batch_size):\n",
    "  #create DataGenerator Object\n",
    "  datagen=ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      zoom_range=0.15,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.15,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode=\"nearest\"\n",
    "      )\n",
    "  \n",
    "  test_datagen= ImageDataGenerator(\n",
    "      rescale=1./255\n",
    "      )\n",
    "  \n",
    "  train_generator = datagen.flow_from_directory(\n",
    "      directory=\"bird/train\",\n",
    "      target_size=(224, 224),\n",
    "      color_mode=\"rgb\",\n",
    "      batch_size=batch_size,\n",
    "      class_mode=\"categorical\",\n",
    "      shuffle=True,\n",
    "      seed=42\n",
    "      )\n",
    "  \n",
    "  val_generator=test_datagen.flow_from_directory(\n",
    "      \"bird/valid\",\n",
    "      target_size=(224,224),\n",
    "      color_mode='rgb',\n",
    "      batch_size=batch_size,\n",
    "      class_mode=\"categorical\", seed = 42,\n",
    "      shuffle=True\n",
    "      )\n",
    "  test_generator = test_datagen.flow_from_directory(\n",
    "      \"bird/test\",\n",
    "      class_mode=\"categorical\",\n",
    "      target_size=(224, 224),\n",
    "      color_mode=\"rgb\",\n",
    "      shuffle=False, seed=42,\n",
    "      batch_size = batch_size\n",
    "      )\n",
    "  \n",
    "  return train_generator , val_generator , test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e5caca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "EP = 32 # Number of Epoches\n",
    "Batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1d1b253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 111, 111, 32  864         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 111, 111, 32  96         ['conv2d[0][0]']                 \n",
      " alization)                     )                                                                 \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 111, 111, 32  0           ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 109, 109, 32  9216        ['activation[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 109, 109, 32  96         ['conv2d_1[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 109, 109, 32  0           ['batch_normalization_1[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 109, 109, 64  18432       ['activation_1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 109, 109, 64  192        ['conv2d_2[0][0]']               \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 109, 109, 64  0           ['batch_normalization_2[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 54, 54, 64)   0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 54, 54, 80)   5120        ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 54, 54, 80)  240         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 54, 54, 80)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 52, 52, 192)  138240      ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 52, 52, 192)  576        ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 52, 52, 192)  0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 25, 25, 192)  0          ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 25, 25, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 25, 25, 64)  192         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 25, 25, 64)   0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 25, 25, 48)   9216        ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 25, 25, 96)   55296       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 25, 25, 48)  144         ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 25, 25, 96)  288         ['conv2d_9[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 25, 25, 48)   0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 25, 25, 96)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 25, 25, 192)  0          ['max_pooling2d_1[0][0]']        \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 25, 25, 64)   12288       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 25, 25, 64)   76800       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 25, 25, 96)   82944       ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 25, 25, 32)   6144        ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_5 (BatchNo  (None, 25, 25, 64)  192         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 25, 25, 64)  192         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 25, 25, 96)  288         ['conv2d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 25, 25, 32)  96          ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 25, 25, 64)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 25, 25, 64)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 25, 25, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " mixed0 (Concatenate)           (None, 25, 25, 256)  0           ['activation_5[0][0]',           \n",
      "                                                                  'activation_7[0][0]',           \n",
      "                                                                  'activation_10[0][0]',          \n",
      "                                                                  'activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 25, 25, 64)   16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 25, 25, 64)  192         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 25, 25, 48)   12288       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 25, 25, 96)   55296       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 25, 25, 48)  144         ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 25, 25, 96)  288         ['conv2d_16[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 25, 25, 48)   0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 25, 25, 256)  0          ['mixed0[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 25, 25, 64)   16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 25, 25, 64)   76800       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 25, 25, 96)   82944       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 25, 25, 64)   16384       ['average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 25, 25, 64)  192         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 25, 25, 64)  192         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 25, 25, 96)  288         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 25, 25, 64)  192         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " mixed1 (Concatenate)           (None, 25, 25, 288)  0           ['activation_12[0][0]',          \n",
      "                                                                  'activation_14[0][0]',          \n",
      "                                                                  'activation_17[0][0]',          \n",
      "                                                                  'activation_18[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 25, 25, 64)   18432       ['mixed1[0][0]']                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " batch_normalization_22 (BatchN  (None, 25, 25, 64)  192         ['conv2d_22[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_22 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_22[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 25, 25, 48)   13824       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 25, 25, 96)   55296       ['activation_22[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_20 (BatchN  (None, 25, 25, 48)  144         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_23 (BatchN  (None, 25, 25, 96)  288         ['conv2d_23[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_20 (Activation)     (None, 25, 25, 48)   0           ['batch_normalization_20[0][0]'] \n",
      "                                                                                                  \n",
      " activation_23 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_23[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 25, 25, 288)  0          ['mixed1[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 25, 25, 64)   18432       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 25, 25, 64)   76800       ['activation_20[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 25, 25, 96)   82944       ['activation_23[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 25, 25, 64)   18432       ['average_pooling2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 25, 25, 64)  192         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_21 (BatchN  (None, 25, 25, 64)  192         ['conv2d_21[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 25, 25, 96)  288         ['conv2d_24[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 25, 25, 64)  192         ['conv2d_25[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_19 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " activation_21 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_21[0][0]'] \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " mixed2 (Concatenate)           (None, 25, 25, 288)  0           ['activation_19[0][0]',          \n",
      "                                                                  'activation_21[0][0]',          \n",
      "                                                                  'activation_24[0][0]',          \n",
      "                                                                  'activation_25[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 25, 25, 64)   18432       ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 25, 25, 64)  192         ['conv2d_27[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 25, 25, 64)   0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 25, 25, 96)   55296       ['activation_27[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 25, 25, 96)  288         ['conv2d_28[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 25, 25, 96)   0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 12, 12, 384)  995328      ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 12, 12, 96)   82944       ['activation_28[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 12, 12, 384)  1152       ['conv2d_26[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 12, 12, 96)  288         ['conv2d_29[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 12, 12, 384)  0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 12, 12, 96)   0           ['batch_normalization_29[0][0]'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 12, 12, 288)  0          ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " mixed3 (Concatenate)           (None, 12, 12, 768)  0           ['activation_26[0][0]',          \n",
      "                                                                  'activation_29[0][0]',          \n",
      "                                                                  'max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 12, 12, 128)  98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_34 (BatchN  (None, 12, 12, 128)  384        ['conv2d_34[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_34 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_34[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 12, 12, 128)  114688      ['activation_34[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_35 (BatchN  (None, 12, 12, 128)  384        ['conv2d_35[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_35 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_35[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 12, 12, 128)  98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 12, 12, 128)  114688      ['activation_35[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 12, 12, 128)  384        ['conv2d_31[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_36 (BatchN  (None, 12, 12, 128)  384        ['conv2d_36[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " activation_36 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_36[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 12, 12, 128)  114688      ['activation_31[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 12, 12, 128)  114688      ['activation_36[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_32 (BatchN  (None, 12, 12, 128)  384        ['conv2d_32[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_37 (BatchN  (None, 12, 12, 128)  384        ['conv2d_37[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_32 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_32[0][0]'] \n",
      "                                                                                                  \n",
      " activation_37 (Activation)     (None, 12, 12, 128)  0           ['batch_normalization_37[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (AveragePo  (None, 12, 12, 768)  0          ['mixed3[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 12, 12, 192)  172032      ['activation_32[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_38 (Conv2D)             (None, 12, 12, 192)  172032      ['activation_37[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_39 (Conv2D)             (None, 12, 12, 192)  147456      ['average_pooling2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 12, 12, 192)  576        ['conv2d_30[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_33 (BatchN  (None, 12, 12, 192)  576        ['conv2d_33[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_38 (BatchN  (None, 12, 12, 192)  576        ['conv2d_38[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_39 (BatchN  (None, 12, 12, 192)  576        ['conv2d_39[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " activation_33 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_33[0][0]'] \n",
      "                                                                                                  \n",
      " activation_38 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_38[0][0]'] \n",
      "                                                                                                  \n",
      " activation_39 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_39[0][0]'] \n",
      "                                                                                                  \n",
      " mixed4 (Concatenate)           (None, 12, 12, 768)  0           ['activation_30[0][0]',          \n",
      "                                                                  'activation_33[0][0]',          \n",
      "                                                                  'activation_38[0][0]',          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  'activation_39[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_44 (Conv2D)             (None, 12, 12, 160)  122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_44 (BatchN  (None, 12, 12, 160)  480        ['conv2d_44[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_44 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_44[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_45 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_44[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_45 (BatchN  (None, 12, 12, 160)  480        ['conv2d_45[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_45 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_45[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_41 (Conv2D)             (None, 12, 12, 160)  122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_46 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_45[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_41 (BatchN  (None, 12, 12, 160)  480        ['conv2d_41[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_46 (BatchN  (None, 12, 12, 160)  480        ['conv2d_46[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_41 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_41[0][0]'] \n",
      "                                                                                                  \n",
      " activation_46 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_46[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_42 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_41[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_47 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_46[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_42 (BatchN  (None, 12, 12, 160)  480        ['conv2d_42[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_47 (BatchN  (None, 12, 12, 160)  480        ['conv2d_47[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_42 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_42[0][0]'] \n",
      "                                                                                                  \n",
      " activation_47 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_47[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_4 (AveragePo  (None, 12, 12, 768)  0          ['mixed4[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_40 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_43 (Conv2D)             (None, 12, 12, 192)  215040      ['activation_42[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_48 (Conv2D)             (None, 12, 12, 192)  215040      ['activation_47[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_49 (Conv2D)             (None, 12, 12, 192)  147456      ['average_pooling2d_4[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_40 (BatchN  (None, 12, 12, 192)  576        ['conv2d_40[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_43 (BatchN  (None, 12, 12, 192)  576        ['conv2d_43[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_48 (BatchN  (None, 12, 12, 192)  576        ['conv2d_48[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_49 (BatchN  (None, 12, 12, 192)  576        ['conv2d_49[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_40 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_40[0][0]'] \n",
      "                                                                                                  \n",
      " activation_43 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_43[0][0]'] \n",
      "                                                                                                  \n",
      " activation_48 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_48[0][0]'] \n",
      "                                                                                                  \n",
      " activation_49 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_49[0][0]'] \n",
      "                                                                                                  \n",
      " mixed5 (Concatenate)           (None, 12, 12, 768)  0           ['activation_40[0][0]',          \n",
      "                                                                  'activation_43[0][0]',          \n",
      "                                                                  'activation_48[0][0]',          \n",
      "                                                                  'activation_49[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_54 (Conv2D)             (None, 12, 12, 160)  122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_54 (BatchN  (None, 12, 12, 160)  480        ['conv2d_54[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_54 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_54[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_55 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_54[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_55 (BatchN  (None, 12, 12, 160)  480        ['conv2d_55[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_55 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_55[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_51 (Conv2D)             (None, 12, 12, 160)  122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_55[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_51 (BatchN  (None, 12, 12, 160)  480        ['conv2d_51[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_56 (BatchN  (None, 12, 12, 160)  480        ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_51 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_51[0][0]'] \n",
      "                                                                                                  \n",
      " activation_56 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_56[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_52 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_51[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 12, 12, 160)  179200      ['activation_56[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_52 (BatchN  (None, 12, 12, 160)  480        ['conv2d_52[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_57 (BatchN  (None, 12, 12, 160)  480        ['conv2d_57[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_52 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_52[0][0]'] \n",
      "                                                                                                  \n",
      " activation_57 (Activation)     (None, 12, 12, 160)  0           ['batch_normalization_57[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_5 (AveragePo  (None, 12, 12, 768)  0          ['mixed5[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_50 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_53 (Conv2D)             (None, 12, 12, 192)  215040      ['activation_52[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 12, 12, 192)  215040      ['activation_57[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 12, 12, 192)  147456      ['average_pooling2d_5[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_50 (BatchN  (None, 12, 12, 192)  576        ['conv2d_50[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_53 (BatchN  (None, 12, 12, 192)  576        ['conv2d_53[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_58 (BatchN  (None, 12, 12, 192)  576        ['conv2d_58[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_59 (BatchN  (None, 12, 12, 192)  576        ['conv2d_59[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_50 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_50[0][0]'] \n",
      "                                                                                                  \n",
      " activation_53 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_53[0][0]'] \n",
      "                                                                                                  \n",
      " activation_58 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_58[0][0]'] \n",
      "                                                                                                  \n",
      " activation_59 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_59[0][0]'] \n",
      "                                                                                                  \n",
      " mixed6 (Concatenate)           (None, 12, 12, 768)  0           ['activation_50[0][0]',          \n",
      "                                                                  'activation_53[0][0]',          \n",
      "                                                                  'activation_58[0][0]',          \n",
      "                                                                  'activation_59[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_64 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_64 (BatchN  (None, 12, 12, 192)  576        ['conv2d_64[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_64 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_64[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_65 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_64[0][0]']          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " batch_normalization_65 (BatchN  (None, 12, 12, 192)  576        ['conv2d_65[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_65 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_65[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_61 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_65[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_61 (BatchN  (None, 12, 12, 192)  576        ['conv2d_61[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_66 (BatchN  (None, 12, 12, 192)  576        ['conv2d_66[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_61 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_61[0][0]'] \n",
      "                                                                                                  \n",
      " activation_66 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_66[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_62 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_61[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_66[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_62 (BatchN  (None, 12, 12, 192)  576        ['conv2d_62[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_67 (BatchN  (None, 12, 12, 192)  576        ['conv2d_67[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_62 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_62[0][0]'] \n",
      "                                                                                                  \n",
      " activation_67 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_67[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_6 (AveragePo  (None, 12, 12, 768)  0          ['mixed6[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_63 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_62[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_67[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 12, 12, 192)  147456      ['average_pooling2d_6[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_60 (BatchN  (None, 12, 12, 192)  576        ['conv2d_60[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_63 (BatchN  (None, 12, 12, 192)  576        ['conv2d_63[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_68 (BatchN  (None, 12, 12, 192)  576        ['conv2d_68[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_69 (BatchN  (None, 12, 12, 192)  576        ['conv2d_69[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_60 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_60[0][0]'] \n",
      "                                                                                                  \n",
      " activation_63 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_63[0][0]'] \n",
      "                                                                                                  \n",
      " activation_68 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_68[0][0]'] \n",
      "                                                                                                  \n",
      " activation_69 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_69[0][0]'] \n",
      "                                                                                                  \n",
      " mixed7 (Concatenate)           (None, 12, 12, 768)  0           ['activation_60[0][0]',          \n",
      "                                                                  'activation_63[0][0]',          \n",
      "                                                                  'activation_68[0][0]',          \n",
      "                                                                  'activation_69[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_72 (BatchN  (None, 12, 12, 192)  576        ['conv2d_72[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_72 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_72[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_72[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_73 (BatchN  (None, 12, 12, 192)  576        ['conv2d_73[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_73 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_73[0][0]'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 12, 12, 192)  147456      ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 12, 12, 192)  258048      ['activation_73[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_70 (BatchN  (None, 12, 12, 192)  576        ['conv2d_70[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_74 (BatchN  (None, 12, 12, 192)  576        ['conv2d_74[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_70 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_70[0][0]'] \n",
      "                                                                                                  \n",
      " activation_74 (Activation)     (None, 12, 12, 192)  0           ['batch_normalization_74[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 5, 5, 320)    552960      ['activation_70[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 5, 5, 192)    331776      ['activation_74[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_71 (BatchN  (None, 5, 5, 320)   960         ['conv2d_71[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_75 (BatchN  (None, 5, 5, 192)   576         ['conv2d_75[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_71 (Activation)     (None, 5, 5, 320)    0           ['batch_normalization_71[0][0]'] \n",
      "                                                                                                  \n",
      " activation_75 (Activation)     (None, 5, 5, 192)    0           ['batch_normalization_75[0][0]'] \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 5, 5, 768)   0           ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " mixed8 (Concatenate)           (None, 5, 5, 1280)   0           ['activation_71[0][0]',          \n",
      "                                                                  'activation_75[0][0]',          \n",
      "                                                                  'max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 5, 5, 448)    573440      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 5, 5, 448)   1344        ['conv2d_80[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_80 (Activation)     (None, 5, 5, 448)    0           ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 5, 5, 384)    491520      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 5, 5, 384)    1548288     ['activation_80[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_77 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_77[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_81[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_77 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_77[0][0]'] \n",
      "                                                                                                  \n",
      " activation_81 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_77[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_77[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_81[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_81[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_7 (AveragePo  (None, 5, 5, 1280)  0           ['mixed8[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 5, 5, 320)    409600      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_78 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_78[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_79 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_79[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_82[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_83 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_83[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 5, 5, 192)    245760      ['average_pooling2d_7[0][0]']    \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_76 (BatchN  (None, 5, 5, 320)   960         ['conv2d_76[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_78 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_78[0][0]'] \n",
      "                                                                                                  \n",
      " activation_79 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_79[0][0]'] \n",
      "                                                                                                  \n",
      " activation_82 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " activation_83 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_83[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_84 (BatchN  (None, 5, 5, 192)   576         ['conv2d_84[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_76 (Activation)     (None, 5, 5, 320)    0           ['batch_normalization_76[0][0]'] \n",
      "                                                                                                  \n",
      " mixed9_0 (Concatenate)         (None, 5, 5, 768)    0           ['activation_78[0][0]',          \n",
      "                                                                  'activation_79[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 5, 5, 768)    0           ['activation_82[0][0]',          \n",
      "                                                                  'activation_83[0][0]']          \n",
      "                                                                                                  \n",
      " activation_84 (Activation)     (None, 5, 5, 192)    0           ['batch_normalization_84[0][0]'] \n",
      "                                                                                                  \n",
      " mixed9 (Concatenate)           (None, 5, 5, 2048)   0           ['activation_76[0][0]',          \n",
      "                                                                  'mixed9_0[0][0]',               \n",
      "                                                                  'concatenate[0][0]',            \n",
      "                                                                  'activation_84[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_89 (Conv2D)             (None, 5, 5, 448)    917504      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_89 (BatchN  (None, 5, 5, 448)   1344        ['conv2d_89[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_89 (Activation)     (None, 5, 5, 448)    0           ['batch_normalization_89[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_86 (Conv2D)             (None, 5, 5, 384)    786432      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_90 (Conv2D)             (None, 5, 5, 384)    1548288     ['activation_89[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_86 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_86[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_90 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_90[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_86 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_86[0][0]'] \n",
      "                                                                                                  \n",
      " activation_90 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_90[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_87 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_86[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_88 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_86[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_91 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_90[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_92 (Conv2D)             (None, 5, 5, 384)    442368      ['activation_90[0][0]']          \n",
      "                                                                                                  \n",
      " average_pooling2d_8 (AveragePo  (None, 5, 5, 2048)  0           ['mixed9[0][0]']                 \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)             (None, 5, 5, 320)    655360      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_87 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_87[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_88 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_88[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_91 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_91[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " batch_normalization_92 (BatchN  (None, 5, 5, 384)   1152        ['conv2d_92[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv2d_93 (Conv2D)             (None, 5, 5, 192)    393216      ['average_pooling2d_8[0][0]']    \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 5, 5, 320)   960         ['conv2d_85[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_87 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_87[0][0]'] \n",
      "                                                                                                  \n",
      " activation_88 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_88[0][0]'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " activation_91 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_91[0][0]'] \n",
      "                                                                                                  \n",
      " activation_92 (Activation)     (None, 5, 5, 384)    0           ['batch_normalization_92[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_93 (BatchN  (None, 5, 5, 192)   576         ['conv2d_93[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_85 (Activation)     (None, 5, 5, 320)    0           ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " mixed9_1 (Concatenate)         (None, 5, 5, 768)    0           ['activation_87[0][0]',          \n",
      "                                                                  'activation_88[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 5, 5, 768)    0           ['activation_91[0][0]',          \n",
      "                                                                  'activation_92[0][0]']          \n",
      "                                                                                                  \n",
      " activation_93 (Activation)     (None, 5, 5, 192)    0           ['batch_normalization_93[0][0]'] \n",
      "                                                                                                  \n",
      " mixed10 (Concatenate)          (None, 5, 5, 2048)   0           ['activation_85[0][0]',          \n",
      "                                                                  'mixed9_1[0][0]',               \n",
      "                                                                  'concatenate_1[0][0]',          \n",
      "                                                                  'activation_93[0][0]']          \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 51200)        0           ['mixed10[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1024)         52429824    ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1024)         1049600     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_94 (BatchN  (None, 1024)        4096        ['dense_1[0][0]']                \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_94 (Activation)     (None, 1024)         0           ['batch_normalization_94[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 1024)         0           ['activation_94[0][0]']          \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          524800      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 400)          205200      ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 76,016,304\n",
      "Trainable params: 54,211,472\n",
      "Non-trainable params: 21,804,832\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "708202f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 58388 images belonging to 400 classes.\n",
      "Found 2000 images belonging to 400 classes.\n",
      "Found 2000 images belonging to 400 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator , val_generator , test_generator = build_data_generator(Batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e30a3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "checkpoint_filepath = 'checkpoint'#'/content/drive/MyDrive/Bird/Checkpoint'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6263114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(\n",
    "    generator=train_generator,\n",
    "    steps_per_epoch= train_generator.n//train_generator.batch_size,\n",
    "    validation_data = val_generator,\n",
    "    validation_steps = val_generator.n//val_generator.batch_size,\n",
    "    epochs=EP,\n",
    "    callbacks = [model_checkpoint_callback,stop_early]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e36d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Visualization\n",
    "# View Accuracy (Training, Validation)\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train_acc\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Validate_acc\")\n",
    "plt.title('Training Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_acc', 'validate_acc'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebfc4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label=\"Train_loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"Validate_loss\")\n",
    "plt.title('Training Accuracy')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'validate_loss'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c950e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_generator.classes\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a341881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "for i in range(1,5):\n",
    "    batch = test_generator.next()\n",
    "    Img_train = (batch[0]*255)\n",
    "    plt.subplot(5,4,i)\n",
    "    plt.imshow(Img_train[0].astype(\"uint8\"))\n",
    "    plt.title('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd8cd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "pred_prob = []\n",
    "pred=model.predict_generator(test_generator)\n",
    "for i in range(len(y_true)):\n",
    "    pred_prob.append(np.array(pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1939ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame(pred_prob)\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79889ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = df_pred.idxmax(axis=1)\n",
    "cm3 = ConfusionMatrix(y_true, list(df_class))\n",
    "cm3.save_html(\"defaultHtml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b29723",
   "metadata": {},
   "source": [
    "Hyperparameter Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71807137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hyper_model(hp):\n",
    "    \n",
    "    #model\n",
    "    base_model  = InceptionV3(weights='imagenet',include_top=False,input_tensor = Input(shape = (224,224,3)))\n",
    "    #architecture\n",
    "    x=base_model.output\n",
    "    x=Flatten()(x)\n",
    "    \n",
    "    layer_num = hp.Int(\"num_layers\", 2, 4)\n",
    "    for i in range(layer_num):\n",
    "        x = Dense(hp.Int(f\"dense_{i}\", min_value=128, max_value=1024, step=128))(x)\n",
    "        x=  Dropout(hp.Float(f\"dropout_{i}\", min_value=0.1, max_value=0.5, step=0.1))(x)\n",
    "        if hp.Boolean(f\"BatchNormalization_{i}\"):\n",
    "            x=  BatchNormalization()(x)\n",
    "        x = Activation(hp.Choice(f\"activation_{i}\", [\"relu\", \"tanh\"]))(x)\n",
    "    preds=Dense(400,activation='softmax')(x)\n",
    "\n",
    "\n",
    "    model=keras.Model(inputs=base_model.input,outputs=preds)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "        \n",
    "    hp_optimizer = hp.Choice(\"optimizer\",values = [\"adam\",\"SGD\",\"RMSprop\"])\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    #default optimizer\n",
    "    optimizer = None\n",
    "    if hp_optimizer == \"adam\":\n",
    "        optimizer = Adam(learning_rate = hp_learning_rate)\n",
    "    elif hp_optimizer == \"SGD\":\n",
    "        optimizer = SGD(learning_rate = hp_learning_rate)\n",
    "    elif hp_optimizer == \"RMSprop\":\n",
    "        optimizer = RMSprop(learning_rate = hp_learning_rate)\n",
    "    hp_loss = hp.Choice(\"loss\",values = [\"categorical_crossentropy\",\"kullback_leibler_divergence\"])\n",
    "    model.compile(optimizer=optimizer,loss=hp_loss,metrics = \"accuracy\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfe01ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = BayesianOptimization(create_hyper_model,\n",
    "                         objective=[\"val_accuracy\"],\n",
    "                             max_trials=6,\n",
    "                             overwrite=True,\n",
    "                             directory='Bayestuner_Dir',\n",
    "                              project_name='Bayes',\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa821786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 Complete [03h 33m 21s]\n",
      "multi_objective: -0.7870000004768372\n",
      "\n",
      "Best multi_objective So Far: -0.8199999928474426\n",
      "Total elapsed time: 18h 11m 26s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(train_generator, \n",
    "             epochs=32, \n",
    "             validation_data=val_generator,\n",
    "             callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa9260b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(2)\n",
    "model = create_hyper_model(best_hps[0])\n",
    "model.save('save_model/best_modelbayes.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446bc58c",
   "metadata": {},
   "source": [
    "ทำ HyperBand Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf99815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Hyperband(create_hyper_model,\n",
    "                         objective=[\"val_accuracy\"],\n",
    "                             overwrite=True,\n",
    "                              max_epochs = 32,\n",
    "                             directory='Hyperband_dir',\n",
    "                              project_name='Hyper',\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "345df805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 53 Complete [00h 24m 49s]\n",
      "multi_objective: -0.025499999523162842\n",
      "\n",
      "Best multi_objective So Far: -0.9304999709129333\n",
      "Total elapsed time: 17h 06m 23s\n",
      "\n",
      "Search: Running Trial #54\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "3                 |2                 |num_layers\n",
      "256               |768               |dense_0\n",
      "0.5               |0.3               |dropout_0\n",
      "False             |True              |BatchNormalization_0\n",
      "tanh              |tanh              |activation_0\n",
      "512               |896               |dense_1\n",
      "0.4               |0.2               |dropout_1\n",
      "True              |False             |BatchNormalization_1\n",
      "tanh              |relu              |activation_1\n",
      "SGD               |adam              |optimizer\n",
      "0.01              |0.0001            |learning_rate\n",
      "kullback_leible...|kullback_leible...|loss\n",
      "512               |768               |dense_2\n",
      "0.2               |0.2               |dropout_2\n",
      "False             |False             |BatchNormalization_2\n",
      "tanh              |relu              |activation_2\n",
      "512               |256               |dense_3\n",
      "0.2               |0.1               |dropout_3\n",
      "False             |False             |BatchNormalization_3\n",
      "tanh              |tanh              |activation_3\n",
      "4                 |32                |tuner/epochs\n",
      "0                 |11                |tuner/initial_epoch\n",
      "2                 |3                 |tuner/bracket\n",
      "0                 |3                 |tuner/round\n",
      "\n",
      "Epoch 1/4\n",
      "457/457 [==============================] - 381s 828ms/step - loss: 5.9212 - accuracy: 0.0140 - val_loss: 5.1004 - val_accuracy: 0.1235\n",
      "Epoch 2/4\n",
      "329/457 [====================>.........] - ETA: 1:43 - loss: 5.5757 - accuracy: 0.0556"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m stop_early \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m             \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m             \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mstop_early\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\engine\\base_tuner.py:179\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 179\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;66;03m# `results` is None indicates user updated oracle in `run_trial()`.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\tuners\\hyperband.py:384\u001b[0m, in \u001b[0;36mHyperband.run_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m     fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtuner/epochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    383\u001b[0m     fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minitial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtuner/initial_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(Hyperband, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mrun_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:294\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    292\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    293\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[1;32m--> 294\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_and_fit_model(trial, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    296\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\engine\\tuner.py:222\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[0;32m    221\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 222\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mfit(hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tuner_utils\u001b[38;5;241m.\u001b[39mconvert_to_metrics_dict(\n\u001b[0;32m    224\u001b[0m     results, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHyperModel.fit()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    225\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras_tuner\\engine\\hypermodel.py:137\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1370\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1363\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1365\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1366\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1367\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1368\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1369\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1370\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1371\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1372\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:916\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    913\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    915\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 916\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    918\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    919\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:948\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    947\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 948\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    952\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2451\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2448\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2449\u001b[0m   (graph_function,\n\u001b[0;32m   2450\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2452\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1859\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1855\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1857\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1858\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1859\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1861\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1862\u001b[0m     args,\n\u001b[0;32m   1863\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1864\u001b[0m     executing_eagerly)\n\u001b[0;32m   1865\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:496\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    495\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    502\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    503\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    505\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    508\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    509\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "tuner.search(train_generator,\n",
    "             validation_data=val_generator,\n",
    "             callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8940b8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hyperband_model():\n",
    "    \n",
    "    #model\n",
    "    base_model  = InceptionV3(weights='imagenet',include_top=False,input_tensor = Input(shape = (224,224,3)))\n",
    "    #architecture\n",
    "    x=base_model.output\n",
    "    x=Flatten()(x)\n",
    "    \n",
    "    x = Dense(738)(x)\n",
    "    x=  Dropout(0.3)(x)\n",
    "    x=  BatchNormalization()(x)\n",
    "    x = Activation(\"tanh\")(x)\n",
    "    x = Dense(896)(x)\n",
    "    x=  Dropout(0.2)(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    preds=Dense(400,activation='softmax')(x)\n",
    "\n",
    "\n",
    "    model=keras.Model(inputs=base_model.input,outputs=preds)\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "        \n",
    "    model.compile(optimizer=Adam(learning_rate = 0.0001),loss=\"kullback_leibler_divergence\",metrics = \"accuracy\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aedd3b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_188 (Conv2D)            (None, 111, 111, 32  864         ['input_3[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_190 (Batch  (None, 111, 111, 32  96         ['conv2d_188[0][0]']             \n",
      " Normalization)                 )                                                                 \n",
      "                                                                                                  \n",
      " activation_193 (Activation)    (None, 111, 111, 32  0           ['batch_normalization_190[0][0]']\n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_189 (Conv2D)            (None, 109, 109, 32  9216        ['activation_193[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_191 (Batch  (None, 109, 109, 32  96         ['conv2d_189[0][0]']             \n",
      " Normalization)                 )                                                                 \n",
      "                                                                                                  \n",
      " activation_194 (Activation)    (None, 109, 109, 32  0           ['batch_normalization_191[0][0]']\n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_190 (Conv2D)            (None, 109, 109, 64  18432       ['activation_194[0][0]']         \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_192 (Batch  (None, 109, 109, 64  192        ['conv2d_190[0][0]']             \n",
      " Normalization)                 )                                                                 \n",
      "                                                                                                  \n",
      " activation_195 (Activation)    (None, 109, 109, 64  0           ['batch_normalization_192[0][0]']\n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_8 (MaxPooling2D)  (None, 54, 54, 64)  0           ['activation_195[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_191 (Conv2D)            (None, 54, 54, 80)   5120        ['max_pooling2d_8[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_193 (Batch  (None, 54, 54, 80)  240         ['conv2d_191[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_196 (Activation)    (None, 54, 54, 80)   0           ['batch_normalization_193[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_192 (Conv2D)            (None, 52, 52, 192)  138240      ['activation_196[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_194 (Batch  (None, 52, 52, 192)  576        ['conv2d_192[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_197 (Activation)    (None, 52, 52, 192)  0           ['batch_normalization_194[0][0]']\n",
      "                                                                                                  \n",
      " max_pooling2d_9 (MaxPooling2D)  (None, 25, 25, 192)  0          ['activation_197[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_196 (Conv2D)            (None, 25, 25, 64)   12288       ['max_pooling2d_9[0][0]']        \n",
      "                                                                                                  \n",
      " batch_normalization_198 (Batch  (None, 25, 25, 64)  192         ['conv2d_196[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_201 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_198[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_194 (Conv2D)            (None, 25, 25, 48)   9216        ['max_pooling2d_9[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_197 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_201[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_196 (Batch  (None, 25, 25, 48)  144         ['conv2d_194[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_199 (Batch  (None, 25, 25, 96)  288         ['conv2d_197[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_199 (Activation)    (None, 25, 25, 48)   0           ['batch_normalization_196[0][0]']\n",
      "                                                                                                  \n",
      " activation_202 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_199[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_18 (AverageP  (None, 25, 25, 192)  0          ['max_pooling2d_9[0][0]']        \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_193 (Conv2D)            (None, 25, 25, 64)   12288       ['max_pooling2d_9[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_195 (Conv2D)            (None, 25, 25, 64)   76800       ['activation_199[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_198 (Conv2D)            (None, 25, 25, 96)   82944       ['activation_202[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_199 (Conv2D)            (None, 25, 25, 32)   6144        ['average_pooling2d_18[0][0]']   \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_195 (Batch  (None, 25, 25, 64)  192         ['conv2d_193[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_197 (Batch  (None, 25, 25, 64)  192         ['conv2d_195[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_200 (Batch  (None, 25, 25, 96)  288         ['conv2d_198[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_201 (Batch  (None, 25, 25, 32)  96          ['conv2d_199[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_198 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_195[0][0]']\n",
      "                                                                                                  \n",
      " activation_200 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_197[0][0]']\n",
      "                                                                                                  \n",
      " activation_203 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_200[0][0]']\n",
      "                                                                                                  \n",
      " activation_204 (Activation)    (None, 25, 25, 32)   0           ['batch_normalization_201[0][0]']\n",
      "                                                                                                  \n",
      " mixed0 (Concatenate)           (None, 25, 25, 256)  0           ['activation_198[0][0]',         \n",
      "                                                                  'activation_200[0][0]',         \n",
      "                                                                  'activation_203[0][0]',         \n",
      "                                                                  'activation_204[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_203 (Conv2D)            (None, 25, 25, 64)   16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_205 (Batch  (None, 25, 25, 64)  192         ['conv2d_203[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_208 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_205[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_201 (Conv2D)            (None, 25, 25, 48)   12288       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_204 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_208[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_203 (Batch  (None, 25, 25, 48)  144         ['conv2d_201[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_206 (Batch  (None, 25, 25, 96)  288         ['conv2d_204[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_206 (Activation)    (None, 25, 25, 48)   0           ['batch_normalization_203[0][0]']\n",
      "                                                                                                  \n",
      " activation_209 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_206[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_19 (AverageP  (None, 25, 25, 256)  0          ['mixed0[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_200 (Conv2D)            (None, 25, 25, 64)   16384       ['mixed0[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_202 (Conv2D)            (None, 25, 25, 64)   76800       ['activation_206[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_205 (Conv2D)            (None, 25, 25, 96)   82944       ['activation_209[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_206 (Conv2D)            (None, 25, 25, 64)   16384       ['average_pooling2d_19[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_202 (Batch  (None, 25, 25, 64)  192         ['conv2d_200[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_204 (Batch  (None, 25, 25, 64)  192         ['conv2d_202[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_207 (Batch  (None, 25, 25, 96)  288         ['conv2d_205[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_208 (Batch  (None, 25, 25, 64)  192         ['conv2d_206[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_205 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_202[0][0]']\n",
      "                                                                                                  \n",
      " activation_207 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_204[0][0]']\n",
      "                                                                                                  \n",
      " activation_210 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_207[0][0]']\n",
      "                                                                                                  \n",
      " activation_211 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_208[0][0]']\n",
      "                                                                                                  \n",
      " mixed1 (Concatenate)           (None, 25, 25, 288)  0           ['activation_205[0][0]',         \n",
      "                                                                  'activation_207[0][0]',         \n",
      "                                                                  'activation_210[0][0]',         \n",
      "                                                                  'activation_211[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_210 (Conv2D)            (None, 25, 25, 64)   18432       ['mixed1[0][0]']                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " batch_normalization_212 (Batch  (None, 25, 25, 64)  192         ['conv2d_210[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_215 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_212[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_208 (Conv2D)            (None, 25, 25, 48)   13824       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_211 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_215[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_210 (Batch  (None, 25, 25, 48)  144         ['conv2d_208[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_213 (Batch  (None, 25, 25, 96)  288         ['conv2d_211[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_213 (Activation)    (None, 25, 25, 48)   0           ['batch_normalization_210[0][0]']\n",
      "                                                                                                  \n",
      " activation_216 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_213[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_20 (AverageP  (None, 25, 25, 288)  0          ['mixed1[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_207 (Conv2D)            (None, 25, 25, 64)   18432       ['mixed1[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_209 (Conv2D)            (None, 25, 25, 64)   76800       ['activation_213[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_212 (Conv2D)            (None, 25, 25, 96)   82944       ['activation_216[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_213 (Conv2D)            (None, 25, 25, 64)   18432       ['average_pooling2d_20[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_209 (Batch  (None, 25, 25, 64)  192         ['conv2d_207[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_211 (Batch  (None, 25, 25, 64)  192         ['conv2d_209[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_214 (Batch  (None, 25, 25, 96)  288         ['conv2d_212[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_215 (Batch  (None, 25, 25, 64)  192         ['conv2d_213[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_212 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_209[0][0]']\n",
      "                                                                                                  \n",
      " activation_214 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_211[0][0]']\n",
      "                                                                                                  \n",
      " activation_217 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_214[0][0]']\n",
      "                                                                                                  \n",
      " activation_218 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_215[0][0]']\n",
      "                                                                                                  \n",
      " mixed2 (Concatenate)           (None, 25, 25, 288)  0           ['activation_212[0][0]',         \n",
      "                                                                  'activation_214[0][0]',         \n",
      "                                                                  'activation_217[0][0]',         \n",
      "                                                                  'activation_218[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_215 (Conv2D)            (None, 25, 25, 64)   18432       ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_217 (Batch  (None, 25, 25, 64)  192         ['conv2d_215[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_220 (Activation)    (None, 25, 25, 64)   0           ['batch_normalization_217[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_216 (Conv2D)            (None, 25, 25, 96)   55296       ['activation_220[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_218 (Batch  (None, 25, 25, 96)  288         ['conv2d_216[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_221 (Activation)    (None, 25, 25, 96)   0           ['batch_normalization_218[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_214 (Conv2D)            (None, 12, 12, 384)  995328      ['mixed2[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_217 (Conv2D)            (None, 12, 12, 96)   82944       ['activation_221[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_216 (Batch  (None, 12, 12, 384)  1152       ['conv2d_214[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_219 (Batch  (None, 12, 12, 96)  288         ['conv2d_217[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_219 (Activation)    (None, 12, 12, 384)  0           ['batch_normalization_216[0][0]']\n",
      "                                                                                                  \n",
      " activation_222 (Activation)    (None, 12, 12, 96)   0           ['batch_normalization_219[0][0]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " max_pooling2d_10 (MaxPooling2D  (None, 12, 12, 288)  0          ['mixed2[0][0]']                 \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " mixed3 (Concatenate)           (None, 12, 12, 768)  0           ['activation_219[0][0]',         \n",
      "                                                                  'activation_222[0][0]',         \n",
      "                                                                  'max_pooling2d_10[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_222 (Conv2D)            (None, 12, 12, 128)  98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_224 (Batch  (None, 12, 12, 128)  384        ['conv2d_222[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_227 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_224[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_223 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_227[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_225 (Batch  (None, 12, 12, 128)  384        ['conv2d_223[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_228 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_225[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_219 (Conv2D)            (None, 12, 12, 128)  98304       ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_224 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_228[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_221 (Batch  (None, 12, 12, 128)  384        ['conv2d_219[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_226 (Batch  (None, 12, 12, 128)  384        ['conv2d_224[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_224 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_221[0][0]']\n",
      "                                                                                                  \n",
      " activation_229 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_226[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_220 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_224[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_225 (Conv2D)            (None, 12, 12, 128)  114688      ['activation_229[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_222 (Batch  (None, 12, 12, 128)  384        ['conv2d_220[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_227 (Batch  (None, 12, 12, 128)  384        ['conv2d_225[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_225 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_222[0][0]']\n",
      "                                                                                                  \n",
      " activation_230 (Activation)    (None, 12, 12, 128)  0           ['batch_normalization_227[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_21 (AverageP  (None, 12, 12, 768)  0          ['mixed3[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_218 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed3[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_221 (Conv2D)            (None, 12, 12, 192)  172032      ['activation_225[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_226 (Conv2D)            (None, 12, 12, 192)  172032      ['activation_230[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_227 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_21[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_220 (Batch  (None, 12, 12, 192)  576        ['conv2d_218[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_223 (Batch  (None, 12, 12, 192)  576        ['conv2d_221[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_228 (Batch  (None, 12, 12, 192)  576        ['conv2d_226[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_229 (Batch  (None, 12, 12, 192)  576        ['conv2d_227[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_223 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_220[0][0]']\n",
      "                                                                                                  \n",
      " activation_226 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_223[0][0]']\n",
      "                                                                                                  \n",
      " activation_231 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_228[0][0]']\n",
      "                                                                                                  \n",
      " activation_232 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_229[0][0]']\n",
      "                                                                                                  \n",
      " mixed4 (Concatenate)           (None, 12, 12, 768)  0           ['activation_223[0][0]',         \n",
      "                                                                  'activation_226[0][0]',         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  'activation_231[0][0]',         \n",
      "                                                                  'activation_232[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_232 (Conv2D)            (None, 12, 12, 160)  122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_234 (Batch  (None, 12, 12, 160)  480        ['conv2d_232[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_237 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_234[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_233 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_237[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_235 (Batch  (None, 12, 12, 160)  480        ['conv2d_233[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_238 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_235[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_229 (Conv2D)            (None, 12, 12, 160)  122880      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_234 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_238[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_231 (Batch  (None, 12, 12, 160)  480        ['conv2d_229[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_236 (Batch  (None, 12, 12, 160)  480        ['conv2d_234[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_234 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_231[0][0]']\n",
      "                                                                                                  \n",
      " activation_239 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_236[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_230 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_234[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_235 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_239[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_232 (Batch  (None, 12, 12, 160)  480        ['conv2d_230[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_237 (Batch  (None, 12, 12, 160)  480        ['conv2d_235[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_235 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_232[0][0]']\n",
      "                                                                                                  \n",
      " activation_240 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_237[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_22 (AverageP  (None, 12, 12, 768)  0          ['mixed4[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_228 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed4[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_231 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_235[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_236 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_240[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_237 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_22[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_230 (Batch  (None, 12, 12, 192)  576        ['conv2d_228[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_233 (Batch  (None, 12, 12, 192)  576        ['conv2d_231[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_238 (Batch  (None, 12, 12, 192)  576        ['conv2d_236[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_239 (Batch  (None, 12, 12, 192)  576        ['conv2d_237[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_233 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_230[0][0]']\n",
      "                                                                                                  \n",
      " activation_236 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_233[0][0]']\n",
      "                                                                                                  \n",
      " activation_241 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_238[0][0]']\n",
      "                                                                                                  \n",
      " activation_242 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_239[0][0]']\n",
      "                                                                                                  \n",
      " mixed5 (Concatenate)           (None, 12, 12, 768)  0           ['activation_233[0][0]',         \n",
      "                                                                  'activation_236[0][0]',         \n",
      "                                                                  'activation_241[0][0]',         \n",
      "                                                                  'activation_242[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_242 (Conv2D)            (None, 12, 12, 160)  122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_normalization_244 (Batch  (None, 12, 12, 160)  480        ['conv2d_242[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_247 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_244[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_243 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_247[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_245 (Batch  (None, 12, 12, 160)  480        ['conv2d_243[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_248 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_245[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_239 (Conv2D)            (None, 12, 12, 160)  122880      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_244 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_248[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_241 (Batch  (None, 12, 12, 160)  480        ['conv2d_239[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_246 (Batch  (None, 12, 12, 160)  480        ['conv2d_244[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_244 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_241[0][0]']\n",
      "                                                                                                  \n",
      " activation_249 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_246[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_240 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_244[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_245 (Conv2D)            (None, 12, 12, 160)  179200      ['activation_249[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_242 (Batch  (None, 12, 12, 160)  480        ['conv2d_240[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_247 (Batch  (None, 12, 12, 160)  480        ['conv2d_245[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_245 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_242[0][0]']\n",
      "                                                                                                  \n",
      " activation_250 (Activation)    (None, 12, 12, 160)  0           ['batch_normalization_247[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_23 (AverageP  (None, 12, 12, 768)  0          ['mixed5[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_238 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed5[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_241 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_245[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_246 (Conv2D)            (None, 12, 12, 192)  215040      ['activation_250[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_247 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_23[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_240 (Batch  (None, 12, 12, 192)  576        ['conv2d_238[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_243 (Batch  (None, 12, 12, 192)  576        ['conv2d_241[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_248 (Batch  (None, 12, 12, 192)  576        ['conv2d_246[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_249 (Batch  (None, 12, 12, 192)  576        ['conv2d_247[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_243 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_240[0][0]']\n",
      "                                                                                                  \n",
      " activation_246 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_243[0][0]']\n",
      "                                                                                                  \n",
      " activation_251 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_248[0][0]']\n",
      "                                                                                                  \n",
      " activation_252 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_249[0][0]']\n",
      "                                                                                                  \n",
      " mixed6 (Concatenate)           (None, 12, 12, 768)  0           ['activation_243[0][0]',         \n",
      "                                                                  'activation_246[0][0]',         \n",
      "                                                                  'activation_251[0][0]',         \n",
      "                                                                  'activation_252[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_252 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_254 (Batch  (None, 12, 12, 192)  576        ['conv2d_252[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_257 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_254[0][0]']\n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_253 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_257[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_255 (Batch  (None, 12, 12, 192)  576        ['conv2d_253[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_258 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_255[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_249 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_254 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_258[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_251 (Batch  (None, 12, 12, 192)  576        ['conv2d_249[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_256 (Batch  (None, 12, 12, 192)  576        ['conv2d_254[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_254 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_251[0][0]']\n",
      "                                                                                                  \n",
      " activation_259 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_256[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_250 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_254[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_255 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_259[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_252 (Batch  (None, 12, 12, 192)  576        ['conv2d_250[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_257 (Batch  (None, 12, 12, 192)  576        ['conv2d_255[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_255 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_252[0][0]']\n",
      "                                                                                                  \n",
      " activation_260 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_257[0][0]']\n",
      "                                                                                                  \n",
      " average_pooling2d_24 (AverageP  (None, 12, 12, 768)  0          ['mixed6[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_248 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed6[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_251 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_255[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_256 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_260[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_257 (Conv2D)            (None, 12, 12, 192)  147456      ['average_pooling2d_24[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_250 (Batch  (None, 12, 12, 192)  576        ['conv2d_248[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_253 (Batch  (None, 12, 12, 192)  576        ['conv2d_251[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_258 (Batch  (None, 12, 12, 192)  576        ['conv2d_256[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_259 (Batch  (None, 12, 12, 192)  576        ['conv2d_257[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_253 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_250[0][0]']\n",
      "                                                                                                  \n",
      " activation_256 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_253[0][0]']\n",
      "                                                                                                  \n",
      " activation_261 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_258[0][0]']\n",
      "                                                                                                  \n",
      " activation_262 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_259[0][0]']\n",
      "                                                                                                  \n",
      " mixed7 (Concatenate)           (None, 12, 12, 768)  0           ['activation_253[0][0]',         \n",
      "                                                                  'activation_256[0][0]',         \n",
      "                                                                  'activation_261[0][0]',         \n",
      "                                                                  'activation_262[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_260 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_262 (Batch  (None, 12, 12, 192)  576        ['conv2d_260[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_265 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_262[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_261 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_265[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_263 (Batch  (None, 12, 12, 192)  576        ['conv2d_261[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " activation_266 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_263[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_258 (Conv2D)            (None, 12, 12, 192)  147456      ['mixed7[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_262 (Conv2D)            (None, 12, 12, 192)  258048      ['activation_266[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_260 (Batch  (None, 12, 12, 192)  576        ['conv2d_258[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_264 (Batch  (None, 12, 12, 192)  576        ['conv2d_262[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_263 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_260[0][0]']\n",
      "                                                                                                  \n",
      " activation_267 (Activation)    (None, 12, 12, 192)  0           ['batch_normalization_264[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_259 (Conv2D)            (None, 5, 5, 320)    552960      ['activation_263[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_263 (Conv2D)            (None, 5, 5, 192)    331776      ['activation_267[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_261 (Batch  (None, 5, 5, 320)   960         ['conv2d_259[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_265 (Batch  (None, 5, 5, 192)   576         ['conv2d_263[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_264 (Activation)    (None, 5, 5, 320)    0           ['batch_normalization_261[0][0]']\n",
      "                                                                                                  \n",
      " activation_268 (Activation)    (None, 5, 5, 192)    0           ['batch_normalization_265[0][0]']\n",
      "                                                                                                  \n",
      " max_pooling2d_11 (MaxPooling2D  (None, 5, 5, 768)   0           ['mixed7[0][0]']                 \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " mixed8 (Concatenate)           (None, 5, 5, 1280)   0           ['activation_264[0][0]',         \n",
      "                                                                  'activation_268[0][0]',         \n",
      "                                                                  'max_pooling2d_11[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_268 (Conv2D)            (None, 5, 5, 448)    573440      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_270 (Batch  (None, 5, 5, 448)   1344        ['conv2d_268[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_273 (Activation)    (None, 5, 5, 448)    0           ['batch_normalization_270[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_265 (Conv2D)            (None, 5, 5, 384)    491520      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_269 (Conv2D)            (None, 5, 5, 384)    1548288     ['activation_273[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_267 (Batch  (None, 5, 5, 384)   1152        ['conv2d_265[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_271 (Batch  (None, 5, 5, 384)   1152        ['conv2d_269[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_270 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_267[0][0]']\n",
      "                                                                                                  \n",
      " activation_274 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_271[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_266 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_270[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_267 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_270[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_270 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_274[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_271 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_274[0][0]']         \n",
      "                                                                                                  \n",
      " average_pooling2d_25 (AverageP  (None, 5, 5, 1280)  0           ['mixed8[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_264 (Conv2D)            (None, 5, 5, 320)    409600      ['mixed8[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_268 (Batch  (None, 5, 5, 384)   1152        ['conv2d_266[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_269 (Batch  (None, 5, 5, 384)   1152        ['conv2d_267[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_272 (Batch  (None, 5, 5, 384)   1152        ['conv2d_270[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_273 (Batch  (None, 5, 5, 384)   1152        ['conv2d_271[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " conv2d_272 (Conv2D)            (None, 5, 5, 192)    245760      ['average_pooling2d_25[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_266 (Batch  (None, 5, 5, 320)   960         ['conv2d_264[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_271 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_268[0][0]']\n",
      "                                                                                                  \n",
      " activation_272 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_269[0][0]']\n",
      "                                                                                                  \n",
      " activation_275 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_272[0][0]']\n",
      "                                                                                                  \n",
      " activation_276 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_273[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_274 (Batch  (None, 5, 5, 192)   576         ['conv2d_272[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_269 (Activation)    (None, 5, 5, 320)    0           ['batch_normalization_266[0][0]']\n",
      "                                                                                                  \n",
      " mixed9_0 (Concatenate)         (None, 5, 5, 768)    0           ['activation_271[0][0]',         \n",
      "                                                                  'activation_272[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 5, 5, 768)    0           ['activation_275[0][0]',         \n",
      "                                                                  'activation_276[0][0]']         \n",
      "                                                                                                  \n",
      " activation_277 (Activation)    (None, 5, 5, 192)    0           ['batch_normalization_274[0][0]']\n",
      "                                                                                                  \n",
      " mixed9 (Concatenate)           (None, 5, 5, 2048)   0           ['activation_269[0][0]',         \n",
      "                                                                  'mixed9_0[0][0]',               \n",
      "                                                                  'concatenate_4[0][0]',          \n",
      "                                                                  'activation_277[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_277 (Conv2D)            (None, 5, 5, 448)    917504      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_279 (Batch  (None, 5, 5, 448)   1344        ['conv2d_277[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_282 (Activation)    (None, 5, 5, 448)    0           ['batch_normalization_279[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_274 (Conv2D)            (None, 5, 5, 384)    786432      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_278 (Conv2D)            (None, 5, 5, 384)    1548288     ['activation_282[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_276 (Batch  (None, 5, 5, 384)   1152        ['conv2d_274[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_280 (Batch  (None, 5, 5, 384)   1152        ['conv2d_278[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_279 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_276[0][0]']\n",
      "                                                                                                  \n",
      " activation_283 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_280[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_275 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_279[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_276 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_279[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_279 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_283[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_280 (Conv2D)            (None, 5, 5, 384)    442368      ['activation_283[0][0]']         \n",
      "                                                                                                  \n",
      " average_pooling2d_26 (AverageP  (None, 5, 5, 2048)  0           ['mixed9[0][0]']                 \n",
      " ooling2D)                                                                                        \n",
      "                                                                                                  \n",
      " conv2d_273 (Conv2D)            (None, 5, 5, 320)    655360      ['mixed9[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_277 (Batch  (None, 5, 5, 384)   1152        ['conv2d_275[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_278 (Batch  (None, 5, 5, 384)   1152        ['conv2d_276[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_281 (Batch  (None, 5, 5, 384)   1152        ['conv2d_279[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_282 (Batch  (None, 5, 5, 384)   1152        ['conv2d_280[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_281 (Conv2D)            (None, 5, 5, 192)    393216      ['average_pooling2d_26[0][0]']   \n",
      "                                                                                                  \n",
      " batch_normalization_275 (Batch  (None, 5, 5, 320)   960         ['conv2d_273[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_280 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_277[0][0]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " activation_281 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_278[0][0]']\n",
      "                                                                                                  \n",
      " activation_284 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_281[0][0]']\n",
      "                                                                                                  \n",
      " activation_285 (Activation)    (None, 5, 5, 384)    0           ['batch_normalization_282[0][0]']\n",
      "                                                                                                  \n",
      " batch_normalization_283 (Batch  (None, 5, 5, 192)   576         ['conv2d_281[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_278 (Activation)    (None, 5, 5, 320)    0           ['batch_normalization_275[0][0]']\n",
      "                                                                                                  \n",
      " mixed9_1 (Concatenate)         (None, 5, 5, 768)    0           ['activation_280[0][0]',         \n",
      "                                                                  'activation_281[0][0]']         \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 5, 5, 768)    0           ['activation_284[0][0]',         \n",
      "                                                                  'activation_285[0][0]']         \n",
      "                                                                                                  \n",
      " activation_286 (Activation)    (None, 5, 5, 192)    0           ['batch_normalization_283[0][0]']\n",
      "                                                                                                  \n",
      " mixed10 (Concatenate)          (None, 5, 5, 2048)   0           ['activation_278[0][0]',         \n",
      "                                                                  'mixed9_1[0][0]',               \n",
      "                                                                  'concatenate_5[0][0]',          \n",
      "                                                                  'activation_286[0][0]']         \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 51200)        0           ['mixed10[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 738)          37786338    ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 738)          0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_284 (Batch  (None, 738)         2952        ['dropout_5[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_287 (Activation)    (None, 738)          0           ['batch_normalization_284[0][0]']\n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 896)          662144      ['activation_287[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 896)          0           ['dense_8[0][0]']                \n",
      "                                                                                                  \n",
      " activation_288 (Activation)    (None, 896)          0           ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 400)          358800      ['activation_288[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 60,613,018\n",
      "Trainable params: 38,808,758\n",
      "Non-trainable params: 21,804,260\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_hyperband_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eab0a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "checkpoint_filepath = 'Hyperbandcheckpoint'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06d5ef46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryu\\AppData\\Local\\Temp\\ipykernel_8744\\1476471888.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  hyperbandhistory = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 3.7834 - accuracy: 0.2844\n",
      "Epoch 1: val_accuracy improved from -inf to 0.62865, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 389s 849ms/step - loss: 3.7834 - accuracy: 0.2844 - val_loss: 1.7230 - val_accuracy: 0.6286\n",
      "Epoch 2/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 2.0770 - accuracy: 0.5361\n",
      "Epoch 2: val_accuracy improved from 0.62865 to 0.76667, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 385s 844ms/step - loss: 2.0770 - accuracy: 0.5361 - val_loss: 0.9673 - val_accuracy: 0.7667\n",
      "Epoch 3/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 1.5616 - accuracy: 0.6224\n",
      "Epoch 3: val_accuracy improved from 0.76667 to 0.82708, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 401s 879ms/step - loss: 1.5616 - accuracy: 0.6224 - val_loss: 0.6915 - val_accuracy: 0.8271\n",
      "Epoch 4/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 1.3252 - accuracy: 0.6674\n",
      "Epoch 4: val_accuracy improved from 0.82708 to 0.85208, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 403s 884ms/step - loss: 1.3252 - accuracy: 0.6674 - val_loss: 0.5590 - val_accuracy: 0.8521\n",
      "Epoch 5/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 1.1775 - accuracy: 0.6966\n",
      "Epoch 5: val_accuracy improved from 0.85208 to 0.86615, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 416s 912ms/step - loss: 1.1775 - accuracy: 0.6966 - val_loss: 0.4900 - val_accuracy: 0.8661\n",
      "Epoch 6/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 1.0819 - accuracy: 0.7179\n",
      "Epoch 6: val_accuracy improved from 0.86615 to 0.87187, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 412s 903ms/step - loss: 1.0819 - accuracy: 0.7179 - val_loss: 0.4406 - val_accuracy: 0.8719\n",
      "Epoch 7/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 1.0077 - accuracy: 0.7343\n",
      "Epoch 7: val_accuracy improved from 0.87187 to 0.88906, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 419s 918ms/step - loss: 1.0077 - accuracy: 0.7343 - val_loss: 0.4060 - val_accuracy: 0.8891\n",
      "Epoch 8/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.9689 - accuracy: 0.7428\n",
      "Epoch 8: val_accuracy improved from 0.88906 to 0.89896, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 423s 927ms/step - loss: 0.9689 - accuracy: 0.7428 - val_loss: 0.3641 - val_accuracy: 0.8990\n",
      "Epoch 9/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.9254 - accuracy: 0.7512\n",
      "Epoch 9: val_accuracy improved from 0.89896 to 0.90365, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 417s 913ms/step - loss: 0.9254 - accuracy: 0.7512 - val_loss: 0.3457 - val_accuracy: 0.9036\n",
      "Epoch 10/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.8766 - accuracy: 0.7616\n",
      "Epoch 10: val_accuracy improved from 0.90365 to 0.91198, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 414s 908ms/step - loss: 0.8766 - accuracy: 0.7616 - val_loss: 0.3190 - val_accuracy: 0.9120\n",
      "Epoch 11/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.8564 - accuracy: 0.7656\n",
      "Epoch 11: val_accuracy improved from 0.91198 to 0.91406, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 428s 938ms/step - loss: 0.8564 - accuracy: 0.7656 - val_loss: 0.3167 - val_accuracy: 0.9141\n",
      "Epoch 12/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.8221 - accuracy: 0.7752\n",
      "Epoch 12: val_accuracy did not improve from 0.91406\n",
      "456/456 [==============================] - 402s 882ms/step - loss: 0.8221 - accuracy: 0.7752 - val_loss: 0.3175 - val_accuracy: 0.9089\n",
      "Epoch 13/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.8011 - accuracy: 0.7793\n",
      "Epoch 13: val_accuracy improved from 0.91406 to 0.91719, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 414s 908ms/step - loss: 0.8011 - accuracy: 0.7793 - val_loss: 0.2985 - val_accuracy: 0.9172\n",
      "Epoch 14/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.7780 - accuracy: 0.7852\n",
      "Epoch 14: val_accuracy did not improve from 0.91719\n",
      "456/456 [==============================] - 398s 873ms/step - loss: 0.7780 - accuracy: 0.7852 - val_loss: 0.2854 - val_accuracy: 0.9151\n",
      "Epoch 15/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.7567 - accuracy: 0.7915\n",
      "Epoch 15: val_accuracy did not improve from 0.91719\n",
      "456/456 [==============================] - 405s 889ms/step - loss: 0.7567 - accuracy: 0.7915 - val_loss: 0.2737 - val_accuracy: 0.9172\n",
      "Epoch 16/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.7367 - accuracy: 0.7962\n",
      "Epoch 16: val_accuracy did not improve from 0.91719\n",
      "456/456 [==============================] - 402s 881ms/step - loss: 0.7367 - accuracy: 0.7962 - val_loss: 0.2768 - val_accuracy: 0.9151\n",
      "Epoch 17/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.7307 - accuracy: 0.7986\n",
      "Epoch 17: val_accuracy improved from 0.91719 to 0.91927, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 493s 1s/step - loss: 0.7307 - accuracy: 0.7986 - val_loss: 0.2709 - val_accuracy: 0.9193\n",
      "Epoch 18/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.7102 - accuracy: 0.8028\n",
      "Epoch 18: val_accuracy improved from 0.91927 to 0.92708, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 481s 1s/step - loss: 0.7102 - accuracy: 0.8028 - val_loss: 0.2537 - val_accuracy: 0.9271\n",
      "Epoch 19/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.6931 - accuracy: 0.8061\n",
      "Epoch 19: val_accuracy did not improve from 0.92708\n",
      "456/456 [==============================] - 400s 876ms/step - loss: 0.6931 - accuracy: 0.8061 - val_loss: 0.2618 - val_accuracy: 0.9214\n",
      "Epoch 20/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.6804 - accuracy: 0.8096\n",
      "Epoch 20: val_accuracy improved from 0.92708 to 0.92865, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 419s 918ms/step - loss: 0.6804 - accuracy: 0.8096 - val_loss: 0.2480 - val_accuracy: 0.9286\n",
      "Epoch 21/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.6674 - accuracy: 0.8117\n",
      "Epoch 21: val_accuracy did not improve from 0.92865\n",
      "456/456 [==============================] - 404s 884ms/step - loss: 0.6674 - accuracy: 0.8117 - val_loss: 0.2419 - val_accuracy: 0.9286\n",
      "Epoch 22/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.6598 - accuracy: 0.8149\n",
      "Epoch 22: val_accuracy improved from 0.92865 to 0.93385, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 427s 935ms/step - loss: 0.6598 - accuracy: 0.8149 - val_loss: 0.2335 - val_accuracy: 0.9339\n",
      "Epoch 23/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.6507 - accuracy: 0.8163\n",
      "Epoch 23: val_accuracy did not improve from 0.93385\n",
      "456/456 [==============================] - 404s 886ms/step - loss: 0.6507 - accuracy: 0.8163 - val_loss: 0.2448 - val_accuracy: 0.9333\n",
      "Epoch 24/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.6370 - accuracy: 0.8203\n",
      "Epoch 24: val_accuracy improved from 0.93385 to 0.94063, saving model to Hyperbandcheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Hyperbandcheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456/456 [==============================] - 420s 922ms/step - loss: 0.6370 - accuracy: 0.8203 - val_loss: 0.2216 - val_accuracy: 0.9406\n",
      "Epoch 25/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.6328 - accuracy: 0.8196\n",
      "Epoch 25: val_accuracy did not improve from 0.94063\n",
      "456/456 [==============================] - 392s 858ms/step - loss: 0.6328 - accuracy: 0.8196 - val_loss: 0.2243 - val_accuracy: 0.9302\n",
      "Epoch 26/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.6205 - accuracy: 0.8238\n",
      "Epoch 26: val_accuracy did not improve from 0.94063\n",
      "456/456 [==============================] - 399s 874ms/step - loss: 0.6205 - accuracy: 0.8238 - val_loss: 0.2119 - val_accuracy: 0.9359\n",
      "Epoch 27/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.6045 - accuracy: 0.8298\n",
      "Epoch 27: val_accuracy did not improve from 0.94063\n",
      "456/456 [==============================] - 394s 863ms/step - loss: 0.6045 - accuracy: 0.8298 - val_loss: 0.2180 - val_accuracy: 0.9365\n",
      "Epoch 28/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.5968 - accuracy: 0.8298\n",
      "Epoch 28: val_accuracy did not improve from 0.94063\n",
      "456/456 [==============================] - 404s 886ms/step - loss: 0.5968 - accuracy: 0.8298 - val_loss: 0.2269 - val_accuracy: 0.9292\n",
      "Epoch 29/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.5999 - accuracy: 0.8297\n",
      "Epoch 29: val_accuracy did not improve from 0.94063\n",
      "456/456 [==============================] - 397s 870ms/step - loss: 0.5999 - accuracy: 0.8297 - val_loss: 0.2212 - val_accuracy: 0.9292\n",
      "Epoch 30/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.5851 - accuracy: 0.8310\n",
      "Epoch 30: val_accuracy did not improve from 0.94063\n",
      "456/456 [==============================] - 436s 957ms/step - loss: 0.5851 - accuracy: 0.8310 - val_loss: 0.2210 - val_accuracy: 0.9307\n",
      "Epoch 31/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.5731 - accuracy: 0.8357\n",
      "Epoch 31: val_accuracy did not improve from 0.94063\n",
      "456/456 [==============================] - 479s 1s/step - loss: 0.5731 - accuracy: 0.8357 - val_loss: 0.2105 - val_accuracy: 0.9365\n",
      "Epoch 32/32\n",
      "456/456 [==============================] - ETA: 0s - loss: 0.5692 - accuracy: 0.8372\n",
      "Epoch 32: val_accuracy did not improve from 0.94063\n",
      "456/456 [==============================] - 395s 866ms/step - loss: 0.5692 - accuracy: 0.8372 - val_loss: 0.2085 - val_accuracy: 0.9370\n"
     ]
    }
   ],
   "source": [
    "hyperbandhistory = model.fit_generator(\n",
    "    generator=train_generator,\n",
    "    steps_per_epoch= train_generator.n//train_generator.batch_size,\n",
    "    validation_data = val_generator,\n",
    "    validation_steps = val_generator.n//val_generator.batch_size,\n",
    "    epochs=EP,\n",
    "    callbacks = [model_checkpoint_callback,stop_early]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7dc05e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('save_model/hyperband_modelsave0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bae71f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2c35986f850>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Performance Visualization\n",
    "# View Accuracy (Training, Validation)\n",
    "plt.plot(hyperbandhistory.history[\"accuracy\"], label=\"Train_acc\")\n",
    "plt.plot(hyperbandhistory.history[\"val_accuracy\"], label=\"Validate_acc\")\n",
    "plt.title('Training Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_acc', 'validate_acc'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a068d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2c35dd65f00>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hyperbandhistory.history['loss'], label=\"Train_loss\")\n",
    "plt.plot(hyperbandhistory.history['val_loss'], label=\"Validate_loss\")\n",
    "plt.title('Training Accuracy')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'validate_loss'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b19a71e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0, ..., 399, 399, 399])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = test_generator.classes\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c77b06aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "for i in range(1,5):\n",
    "    batch = test_generator.next()\n",
    "    Img_train = (batch[0]*255)\n",
    "    plt.subplot(5,4,i)\n",
    "    plt.imshow(Img_train[0].astype(\"uint8\"))\n",
    "    plt.title('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6243151d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryu\\AppData\\Local\\Temp\\ipykernel_8744\\4136271206.py:3: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  pred=model.predict_generator(test_generator)\n"
     ]
    }
   ],
   "source": [
    "test_generator.reset()\n",
    "pred_prob = []\n",
    "pred=model.predict_generator(test_generator)\n",
    "for i in range(len(y_true)):\n",
    "    pred_prob.append(np.array(pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7fc8b424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.972239e-01</td>\n",
       "      <td>4.118325e-07</td>\n",
       "      <td>7.834086e-09</td>\n",
       "      <td>1.909348e-10</td>\n",
       "      <td>4.072343e-08</td>\n",
       "      <td>1.111216e-04</td>\n",
       "      <td>1.383186e-10</td>\n",
       "      <td>1.724553e-11</td>\n",
       "      <td>3.576082e-03</td>\n",
       "      <td>8.706433e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>3.595939e-06</td>\n",
       "      <td>7.003432e-10</td>\n",
       "      <td>7.437088e-11</td>\n",
       "      <td>2.873474e-10</td>\n",
       "      <td>1.683895e-10</td>\n",
       "      <td>1.120958e-06</td>\n",
       "      <td>4.747245e-10</td>\n",
       "      <td>6.393783e-05</td>\n",
       "      <td>2.801036e-07</td>\n",
       "      <td>2.058767e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.968646e-01</td>\n",
       "      <td>4.958514e-06</td>\n",
       "      <td>9.161965e-10</td>\n",
       "      <td>5.469471e-10</td>\n",
       "      <td>6.357700e-05</td>\n",
       "      <td>5.592098e-04</td>\n",
       "      <td>3.332993e-09</td>\n",
       "      <td>7.854923e-08</td>\n",
       "      <td>4.544512e-04</td>\n",
       "      <td>3.101923e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.966963e-05</td>\n",
       "      <td>3.005664e-08</td>\n",
       "      <td>2.281387e-08</td>\n",
       "      <td>5.599697e-07</td>\n",
       "      <td>2.800136e-10</td>\n",
       "      <td>5.225917e-07</td>\n",
       "      <td>6.121202e-08</td>\n",
       "      <td>2.807187e-02</td>\n",
       "      <td>1.039715e-07</td>\n",
       "      <td>1.789097e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.330798e-01</td>\n",
       "      <td>2.662246e-06</td>\n",
       "      <td>1.138484e-07</td>\n",
       "      <td>1.106305e-08</td>\n",
       "      <td>1.498301e-05</td>\n",
       "      <td>1.569598e-05</td>\n",
       "      <td>1.689582e-10</td>\n",
       "      <td>1.011397e-08</td>\n",
       "      <td>3.977865e-04</td>\n",
       "      <td>5.722264e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.984354e-06</td>\n",
       "      <td>1.912284e-07</td>\n",
       "      <td>6.111468e-07</td>\n",
       "      <td>1.238916e-06</td>\n",
       "      <td>5.546046e-10</td>\n",
       "      <td>9.188429e-07</td>\n",
       "      <td>1.790266e-08</td>\n",
       "      <td>1.641520e-02</td>\n",
       "      <td>3.488960e-06</td>\n",
       "      <td>5.577636e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.213212e-03</td>\n",
       "      <td>2.437622e-08</td>\n",
       "      <td>1.161440e-10</td>\n",
       "      <td>1.985552e-12</td>\n",
       "      <td>1.225732e-08</td>\n",
       "      <td>5.463820e-08</td>\n",
       "      <td>4.126589e-12</td>\n",
       "      <td>1.401166e-12</td>\n",
       "      <td>2.187440e-07</td>\n",
       "      <td>1.309644e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>1.265224e-10</td>\n",
       "      <td>5.824749e-10</td>\n",
       "      <td>1.834727e-10</td>\n",
       "      <td>3.845491e-10</td>\n",
       "      <td>3.032578e-11</td>\n",
       "      <td>2.615474e-10</td>\n",
       "      <td>9.607449e-11</td>\n",
       "      <td>3.609545e-09</td>\n",
       "      <td>1.274581e-09</td>\n",
       "      <td>2.281588e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.834055e-01</td>\n",
       "      <td>1.179215e-08</td>\n",
       "      <td>1.186466e-09</td>\n",
       "      <td>2.107347e-11</td>\n",
       "      <td>6.460049e-10</td>\n",
       "      <td>6.551555e-05</td>\n",
       "      <td>1.703772e-10</td>\n",
       "      <td>3.555954e-10</td>\n",
       "      <td>1.013716e-04</td>\n",
       "      <td>3.146352e-11</td>\n",
       "      <td>...</td>\n",
       "      <td>2.813590e-06</td>\n",
       "      <td>1.213371e-11</td>\n",
       "      <td>3.103971e-11</td>\n",
       "      <td>1.172822e-09</td>\n",
       "      <td>2.170984e-11</td>\n",
       "      <td>2.688575e-07</td>\n",
       "      <td>2.007045e-09</td>\n",
       "      <td>2.656276e-04</td>\n",
       "      <td>7.476232e-09</td>\n",
       "      <td>5.921814e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>1.172350e-13</td>\n",
       "      <td>6.296837e-11</td>\n",
       "      <td>7.145251e-10</td>\n",
       "      <td>1.195112e-10</td>\n",
       "      <td>2.022456e-11</td>\n",
       "      <td>1.345512e-09</td>\n",
       "      <td>9.505123e-14</td>\n",
       "      <td>3.658186e-12</td>\n",
       "      <td>2.631846e-14</td>\n",
       "      <td>9.449994e-13</td>\n",
       "      <td>...</td>\n",
       "      <td>1.040524e-09</td>\n",
       "      <td>3.445594e-11</td>\n",
       "      <td>1.198689e-10</td>\n",
       "      <td>1.059163e-09</td>\n",
       "      <td>9.096022e-12</td>\n",
       "      <td>4.301890e-07</td>\n",
       "      <td>3.215260e-13</td>\n",
       "      <td>3.485932e-10</td>\n",
       "      <td>9.580858e-07</td>\n",
       "      <td>9.998773e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>3.361829e-11</td>\n",
       "      <td>5.725676e-09</td>\n",
       "      <td>7.907421e-10</td>\n",
       "      <td>4.831246e-11</td>\n",
       "      <td>2.415260e-09</td>\n",
       "      <td>2.614116e-07</td>\n",
       "      <td>7.613951e-11</td>\n",
       "      <td>3.374291e-11</td>\n",
       "      <td>1.628692e-09</td>\n",
       "      <td>2.804780e-11</td>\n",
       "      <td>...</td>\n",
       "      <td>5.452963e-09</td>\n",
       "      <td>1.602448e-09</td>\n",
       "      <td>1.151736e-08</td>\n",
       "      <td>5.324982e-08</td>\n",
       "      <td>2.121225e-11</td>\n",
       "      <td>3.043402e-09</td>\n",
       "      <td>1.507434e-11</td>\n",
       "      <td>5.456389e-08</td>\n",
       "      <td>4.790314e-07</td>\n",
       "      <td>9.966454e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1.696374e-11</td>\n",
       "      <td>2.636294e-09</td>\n",
       "      <td>4.248848e-09</td>\n",
       "      <td>7.313559e-11</td>\n",
       "      <td>1.557817e-09</td>\n",
       "      <td>1.413628e-07</td>\n",
       "      <td>3.875568e-10</td>\n",
       "      <td>4.616695e-12</td>\n",
       "      <td>4.189849e-10</td>\n",
       "      <td>1.803550e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>2.307069e-09</td>\n",
       "      <td>7.230978e-10</td>\n",
       "      <td>4.371963e-10</td>\n",
       "      <td>1.939962e-09</td>\n",
       "      <td>2.453219e-11</td>\n",
       "      <td>5.950736e-08</td>\n",
       "      <td>1.012936e-11</td>\n",
       "      <td>9.858466e-07</td>\n",
       "      <td>8.647864e-06</td>\n",
       "      <td>9.981508e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>3.882630e-05</td>\n",
       "      <td>5.424066e-06</td>\n",
       "      <td>5.158725e-07</td>\n",
       "      <td>1.531201e-09</td>\n",
       "      <td>3.578950e-08</td>\n",
       "      <td>2.980473e-02</td>\n",
       "      <td>8.964940e-06</td>\n",
       "      <td>1.359491e-06</td>\n",
       "      <td>1.589873e-03</td>\n",
       "      <td>4.676945e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>8.434139e-07</td>\n",
       "      <td>5.230330e-06</td>\n",
       "      <td>1.792259e-07</td>\n",
       "      <td>1.166984e-07</td>\n",
       "      <td>2.418193e-06</td>\n",
       "      <td>2.097602e-07</td>\n",
       "      <td>3.493590e-08</td>\n",
       "      <td>1.697272e-06</td>\n",
       "      <td>6.133685e-09</td>\n",
       "      <td>2.914565e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>3.387244e-15</td>\n",
       "      <td>8.828086e-12</td>\n",
       "      <td>6.658256e-10</td>\n",
       "      <td>1.051268e-12</td>\n",
       "      <td>7.176005e-12</td>\n",
       "      <td>6.024269e-10</td>\n",
       "      <td>3.422749e-11</td>\n",
       "      <td>6.573346e-14</td>\n",
       "      <td>3.200865e-13</td>\n",
       "      <td>1.146110e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>8.105964e-11</td>\n",
       "      <td>5.276486e-10</td>\n",
       "      <td>3.537154e-12</td>\n",
       "      <td>4.046715e-11</td>\n",
       "      <td>2.500423e-12</td>\n",
       "      <td>2.628915e-09</td>\n",
       "      <td>9.659516e-13</td>\n",
       "      <td>3.855324e-11</td>\n",
       "      <td>5.604286e-08</td>\n",
       "      <td>9.999353e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0             1             2             3             4    \\\n",
       "0     8.972239e-01  4.118325e-07  7.834086e-09  1.909348e-10  4.072343e-08   \n",
       "1     2.968646e-01  4.958514e-06  9.161965e-10  5.469471e-10  6.357700e-05   \n",
       "2     5.330798e-01  2.662246e-06  1.138484e-07  1.106305e-08  1.498301e-05   \n",
       "3     5.213212e-03  2.437622e-08  1.161440e-10  1.985552e-12  1.225732e-08   \n",
       "4     9.834055e-01  1.179215e-08  1.186466e-09  2.107347e-11  6.460049e-10   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1995  1.172350e-13  6.296837e-11  7.145251e-10  1.195112e-10  2.022456e-11   \n",
       "1996  3.361829e-11  5.725676e-09  7.907421e-10  4.831246e-11  2.415260e-09   \n",
       "1997  1.696374e-11  2.636294e-09  4.248848e-09  7.313559e-11  1.557817e-09   \n",
       "1998  3.882630e-05  5.424066e-06  5.158725e-07  1.531201e-09  3.578950e-08   \n",
       "1999  3.387244e-15  8.828086e-12  6.658256e-10  1.051268e-12  7.176005e-12   \n",
       "\n",
       "               5             6             7             8             9    \\\n",
       "0     1.111216e-04  1.383186e-10  1.724553e-11  3.576082e-03  8.706433e-12   \n",
       "1     5.592098e-04  3.332993e-09  7.854923e-08  4.544512e-04  3.101923e-06   \n",
       "2     1.569598e-05  1.689582e-10  1.011397e-08  3.977865e-04  5.722264e-09   \n",
       "3     5.463820e-08  4.126589e-12  1.401166e-12  2.187440e-07  1.309644e-12   \n",
       "4     6.551555e-05  1.703772e-10  3.555954e-10  1.013716e-04  3.146352e-11   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1995  1.345512e-09  9.505123e-14  3.658186e-12  2.631846e-14  9.449994e-13   \n",
       "1996  2.614116e-07  7.613951e-11  3.374291e-11  1.628692e-09  2.804780e-11   \n",
       "1997  1.413628e-07  3.875568e-10  4.616695e-12  4.189849e-10  1.803550e-12   \n",
       "1998  2.980473e-02  8.964940e-06  1.359491e-06  1.589873e-03  4.676945e-09   \n",
       "1999  6.024269e-10  3.422749e-11  6.573346e-14  3.200865e-13  1.146110e-12   \n",
       "\n",
       "      ...           390           391           392           393  \\\n",
       "0     ...  3.595939e-06  7.003432e-10  7.437088e-11  2.873474e-10   \n",
       "1     ...  1.966963e-05  3.005664e-08  2.281387e-08  5.599697e-07   \n",
       "2     ...  1.984354e-06  1.912284e-07  6.111468e-07  1.238916e-06   \n",
       "3     ...  1.265224e-10  5.824749e-10  1.834727e-10  3.845491e-10   \n",
       "4     ...  2.813590e-06  1.213371e-11  3.103971e-11  1.172822e-09   \n",
       "...   ...           ...           ...           ...           ...   \n",
       "1995  ...  1.040524e-09  3.445594e-11  1.198689e-10  1.059163e-09   \n",
       "1996  ...  5.452963e-09  1.602448e-09  1.151736e-08  5.324982e-08   \n",
       "1997  ...  2.307069e-09  7.230978e-10  4.371963e-10  1.939962e-09   \n",
       "1998  ...  8.434139e-07  5.230330e-06  1.792259e-07  1.166984e-07   \n",
       "1999  ...  8.105964e-11  5.276486e-10  3.537154e-12  4.046715e-11   \n",
       "\n",
       "               394           395           396           397           398  \\\n",
       "0     1.683895e-10  1.120958e-06  4.747245e-10  6.393783e-05  2.801036e-07   \n",
       "1     2.800136e-10  5.225917e-07  6.121202e-08  2.807187e-02  1.039715e-07   \n",
       "2     5.546046e-10  9.188429e-07  1.790266e-08  1.641520e-02  3.488960e-06   \n",
       "3     3.032578e-11  2.615474e-10  9.607449e-11  3.609545e-09  1.274581e-09   \n",
       "4     2.170984e-11  2.688575e-07  2.007045e-09  2.656276e-04  7.476232e-09   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1995  9.096022e-12  4.301890e-07  3.215260e-13  3.485932e-10  9.580858e-07   \n",
       "1996  2.121225e-11  3.043402e-09  1.507434e-11  5.456389e-08  4.790314e-07   \n",
       "1997  2.453219e-11  5.950736e-08  1.012936e-11  9.858466e-07  8.647864e-06   \n",
       "1998  2.418193e-06  2.097602e-07  3.493590e-08  1.697272e-06  6.133685e-09   \n",
       "1999  2.500423e-12  2.628915e-09  9.659516e-13  3.855324e-11  5.604286e-08   \n",
       "\n",
       "               399  \n",
       "0     2.058767e-07  \n",
       "1     1.789097e-08  \n",
       "2     5.577636e-07  \n",
       "3     2.281588e-09  \n",
       "4     5.921814e-10  \n",
       "...            ...  \n",
       "1995  9.998773e-01  \n",
       "1996  9.966454e-01  \n",
       "1997  9.981508e-01  \n",
       "1998  2.914565e-01  \n",
       "1999  9.999353e-01  \n",
       "\n",
       "[2000 rows x 400 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = pd.DataFrame(pred_prob)\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46107790",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = df_pred.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a1e3c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.60      0.75         5\n",
      "           1       1.00      0.80      0.89         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       1.00      0.80      0.89         5\n",
      "           5       0.83      1.00      0.91         5\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      1.00      1.00         5\n",
      "           9       1.00      1.00      1.00         5\n",
      "          10       1.00      1.00      1.00         5\n",
      "          11       1.00      0.20      0.33         5\n",
      "          12       1.00      1.00      1.00         5\n",
      "          13       1.00      1.00      1.00         5\n",
      "          14       1.00      1.00      1.00         5\n",
      "          15       1.00      1.00      1.00         5\n",
      "          16       0.71      1.00      0.83         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       0.71      1.00      0.83         5\n",
      "          19       1.00      0.60      0.75         5\n",
      "          20       1.00      0.80      0.89         5\n",
      "          21       1.00      1.00      1.00         5\n",
      "          22       0.83      1.00      0.91         5\n",
      "          23       0.83      1.00      0.91         5\n",
      "          24       1.00      0.60      0.75         5\n",
      "          25       0.71      1.00      0.83         5\n",
      "          26       1.00      1.00      1.00         5\n",
      "          27       0.80      0.80      0.80         5\n",
      "          28       1.00      0.80      0.89         5\n",
      "          29       1.00      1.00      1.00         5\n",
      "          30       1.00      1.00      1.00         5\n",
      "          31       1.00      0.80      0.89         5\n",
      "          32       1.00      1.00      1.00         5\n",
      "          33       0.38      0.60      0.46         5\n",
      "          34       1.00      1.00      1.00         5\n",
      "          35       1.00      1.00      1.00         5\n",
      "          36       0.83      1.00      0.91         5\n",
      "          37       1.00      1.00      1.00         5\n",
      "          38       0.83      1.00      0.91         5\n",
      "          39       1.00      0.80      0.89         5\n",
      "          40       1.00      1.00      1.00         5\n",
      "          41       0.83      1.00      0.91         5\n",
      "          42       1.00      1.00      1.00         5\n",
      "          43       1.00      1.00      1.00         5\n",
      "          44       0.83      1.00      0.91         5\n",
      "          45       1.00      1.00      1.00         5\n",
      "          46       1.00      1.00      1.00         5\n",
      "          47       1.00      1.00      1.00         5\n",
      "          48       1.00      1.00      1.00         5\n",
      "          49       1.00      1.00      1.00         5\n",
      "          50       1.00      1.00      1.00         5\n",
      "          51       1.00      1.00      1.00         5\n",
      "          52       0.83      1.00      0.91         5\n",
      "          53       1.00      1.00      1.00         5\n",
      "          54       1.00      1.00      1.00         5\n",
      "          55       1.00      1.00      1.00         5\n",
      "          56       0.83      1.00      0.91         5\n",
      "          57       1.00      1.00      1.00         5\n",
      "          58       1.00      1.00      1.00         5\n",
      "          59       0.83      1.00      0.91         5\n",
      "          60       0.80      0.80      0.80         5\n",
      "          61       0.83      1.00      0.91         5\n",
      "          62       1.00      1.00      1.00         5\n",
      "          63       1.00      1.00      1.00         5\n",
      "          64       1.00      0.80      0.89         5\n",
      "          65       1.00      1.00      1.00         5\n",
      "          66       1.00      0.80      0.89         5\n",
      "          67       1.00      1.00      1.00         5\n",
      "          68       1.00      1.00      1.00         5\n",
      "          69       1.00      1.00      1.00         5\n",
      "          70       1.00      1.00      1.00         5\n",
      "          71       0.80      0.80      0.80         5\n",
      "          72       1.00      1.00      1.00         5\n",
      "          73       1.00      0.80      0.89         5\n",
      "          74       1.00      0.80      0.89         5\n",
      "          75       1.00      1.00      1.00         5\n",
      "          76       1.00      1.00      1.00         5\n",
      "          77       0.83      1.00      0.91         5\n",
      "          78       1.00      1.00      1.00         5\n",
      "          79       1.00      0.60      0.75         5\n",
      "          80       1.00      1.00      1.00         5\n",
      "          81       1.00      1.00      1.00         5\n",
      "          82       1.00      1.00      1.00         5\n",
      "          83       0.83      1.00      0.91         5\n",
      "          84       0.83      1.00      0.91         5\n",
      "          85       1.00      1.00      1.00         5\n",
      "          86       1.00      0.80      0.89         5\n",
      "          87       0.83      1.00      0.91         5\n",
      "          88       0.83      1.00      0.91         5\n",
      "          89       1.00      0.80      0.89         5\n",
      "          90       1.00      1.00      1.00         5\n",
      "          91       0.83      1.00      0.91         5\n",
      "          92       1.00      1.00      1.00         5\n",
      "          93       1.00      0.80      0.89         5\n",
      "          94       0.83      1.00      0.91         5\n",
      "          95       1.00      1.00      1.00         5\n",
      "          96       1.00      1.00      1.00         5\n",
      "          97       0.83      1.00      0.91         5\n",
      "          98       1.00      0.80      0.89         5\n",
      "          99       0.83      1.00      0.91         5\n",
      "         100       1.00      0.80      0.89         5\n",
      "         101       1.00      1.00      1.00         5\n",
      "         102       0.83      1.00      0.91         5\n",
      "         103       1.00      1.00      1.00         5\n",
      "         104       1.00      1.00      1.00         5\n",
      "         105       0.80      0.80      0.80         5\n",
      "         106       1.00      1.00      1.00         5\n",
      "         107       1.00      0.80      0.89         5\n",
      "         108       1.00      1.00      1.00         5\n",
      "         109       1.00      1.00      1.00         5\n",
      "         110       1.00      1.00      1.00         5\n",
      "         111       1.00      0.60      0.75         5\n",
      "         112       1.00      1.00      1.00         5\n",
      "         113       1.00      1.00      1.00         5\n",
      "         114       1.00      1.00      1.00         5\n",
      "         115       1.00      1.00      1.00         5\n",
      "         116       1.00      1.00      1.00         5\n",
      "         117       1.00      1.00      1.00         5\n",
      "         118       1.00      1.00      1.00         5\n",
      "         119       1.00      1.00      1.00         5\n",
      "         120       1.00      1.00      1.00         5\n",
      "         121       0.75      0.60      0.67         5\n",
      "         122       1.00      1.00      1.00         5\n",
      "         123       0.83      1.00      0.91         5\n",
      "         124       1.00      1.00      1.00         5\n",
      "         125       1.00      1.00      1.00         5\n",
      "         126       1.00      1.00      1.00         5\n",
      "         127       1.00      1.00      1.00         5\n",
      "         128       1.00      1.00      1.00         5\n",
      "         129       1.00      1.00      1.00         5\n",
      "         130       1.00      1.00      1.00         5\n",
      "         131       1.00      1.00      1.00         5\n",
      "         132       1.00      1.00      1.00         5\n",
      "         133       1.00      1.00      1.00         5\n",
      "         134       1.00      1.00      1.00         5\n",
      "         135       1.00      0.80      0.89         5\n",
      "         136       1.00      1.00      1.00         5\n",
      "         137       1.00      0.80      0.89         5\n",
      "         138       1.00      1.00      1.00         5\n",
      "         139       0.83      1.00      0.91         5\n",
      "         140       0.60      0.60      0.60         5\n",
      "         141       1.00      1.00      1.00         5\n",
      "         142       1.00      1.00      1.00         5\n",
      "         143       1.00      1.00      1.00         5\n",
      "         144       1.00      1.00      1.00         5\n",
      "         145       0.83      1.00      0.91         5\n",
      "         146       1.00      1.00      1.00         5\n",
      "         147       1.00      1.00      1.00         5\n",
      "         148       1.00      1.00      1.00         5\n",
      "         149       0.83      1.00      0.91         5\n",
      "         150       1.00      1.00      1.00         5\n",
      "         151       1.00      1.00      1.00         5\n",
      "         152       1.00      1.00      1.00         5\n",
      "         153       1.00      0.80      0.89         5\n",
      "         154       1.00      1.00      1.00         5\n",
      "         155       1.00      1.00      1.00         5\n",
      "         156       1.00      0.80      0.89         5\n",
      "         157       1.00      1.00      1.00         5\n",
      "         158       1.00      1.00      1.00         5\n",
      "         159       1.00      0.80      0.89         5\n",
      "         160       0.71      1.00      0.83         5\n",
      "         161       1.00      1.00      1.00         5\n",
      "         162       0.83      1.00      0.91         5\n",
      "         163       1.00      1.00      1.00         5\n",
      "         164       1.00      1.00      1.00         5\n",
      "         165       1.00      1.00      1.00         5\n",
      "         166       1.00      0.80      0.89         5\n",
      "         167       1.00      1.00      1.00         5\n",
      "         168       1.00      1.00      1.00         5\n",
      "         169       1.00      1.00      1.00         5\n",
      "         170       1.00      0.80      0.89         5\n",
      "         171       0.83      1.00      0.91         5\n",
      "         172       0.71      1.00      0.83         5\n",
      "         173       1.00      1.00      1.00         5\n",
      "         174       1.00      1.00      1.00         5\n",
      "         175       1.00      1.00      1.00         5\n",
      "         176       1.00      0.80      0.89         5\n",
      "         177       1.00      1.00      1.00         5\n",
      "         178       1.00      1.00      1.00         5\n",
      "         179       1.00      0.80      0.89         5\n",
      "         180       1.00      0.80      0.89         5\n",
      "         181       1.00      1.00      1.00         5\n",
      "         182       1.00      1.00      1.00         5\n",
      "         183       1.00      1.00      1.00         5\n",
      "         184       1.00      0.80      0.89         5\n",
      "         185       1.00      0.80      0.89         5\n",
      "         186       1.00      1.00      1.00         5\n",
      "         187       1.00      1.00      1.00         5\n",
      "         188       1.00      1.00      1.00         5\n",
      "         189       1.00      1.00      1.00         5\n",
      "         190       1.00      1.00      1.00         5\n",
      "         191       0.80      0.80      0.80         5\n",
      "         192       1.00      1.00      1.00         5\n",
      "         193       1.00      1.00      1.00         5\n",
      "         194       1.00      0.80      0.89         5\n",
      "         195       0.67      0.80      0.73         5\n",
      "         196       1.00      0.60      0.75         5\n",
      "         197       1.00      1.00      1.00         5\n",
      "         198       1.00      0.80      0.89         5\n",
      "         199       1.00      0.80      0.89         5\n",
      "         200       1.00      1.00      1.00         5\n",
      "         201       1.00      1.00      1.00         5\n",
      "         202       1.00      0.80      0.89         5\n",
      "         203       0.83      1.00      0.91         5\n",
      "         204       1.00      1.00      1.00         5\n",
      "         205       1.00      1.00      1.00         5\n",
      "         206       1.00      1.00      1.00         5\n",
      "         207       1.00      1.00      1.00         5\n",
      "         208       1.00      1.00      1.00         5\n",
      "         209       1.00      1.00      1.00         5\n",
      "         210       0.83      1.00      0.91         5\n",
      "         211       1.00      1.00      1.00         5\n",
      "         212       1.00      1.00      1.00         5\n",
      "         213       1.00      1.00      1.00         5\n",
      "         214       0.71      1.00      0.83         5\n",
      "         215       1.00      0.80      0.89         5\n",
      "         216       1.00      1.00      1.00         5\n",
      "         217       1.00      1.00      1.00         5\n",
      "         218       1.00      1.00      1.00         5\n",
      "         219       1.00      1.00      1.00         5\n",
      "         220       1.00      1.00      1.00         5\n",
      "         221       1.00      1.00      1.00         5\n",
      "         222       1.00      1.00      1.00         5\n",
      "         223       0.83      1.00      0.91         5\n",
      "         224       0.71      1.00      0.83         5\n",
      "         225       1.00      1.00      1.00         5\n",
      "         226       1.00      1.00      1.00         5\n",
      "         227       1.00      1.00      1.00         5\n",
      "         228       1.00      1.00      1.00         5\n",
      "         229       0.75      0.60      0.67         5\n",
      "         230       1.00      1.00      1.00         5\n",
      "         231       1.00      1.00      1.00         5\n",
      "         232       1.00      1.00      1.00         5\n",
      "         233       1.00      1.00      1.00         5\n",
      "         234       1.00      1.00      1.00         5\n",
      "         235       1.00      1.00      1.00         5\n",
      "         236       1.00      1.00      1.00         5\n",
      "         237       0.83      1.00      0.91         5\n",
      "         238       1.00      1.00      1.00         5\n",
      "         239       1.00      1.00      1.00         5\n",
      "         240       0.83      1.00      0.91         5\n",
      "         241       1.00      1.00      1.00         5\n",
      "         242       1.00      1.00      1.00         5\n",
      "         243       1.00      1.00      1.00         5\n",
      "         244       1.00      1.00      1.00         5\n",
      "         245       1.00      1.00      1.00         5\n",
      "         246       0.83      1.00      0.91         5\n",
      "         247       1.00      1.00      1.00         5\n",
      "         248       1.00      1.00      1.00         5\n",
      "         249       0.83      1.00      0.91         5\n",
      "         250       0.83      1.00      0.91         5\n",
      "         251       1.00      0.80      0.89         5\n",
      "         252       1.00      1.00      1.00         5\n",
      "         253       0.83      1.00      0.91         5\n",
      "         254       1.00      1.00      1.00         5\n",
      "         255       1.00      1.00      1.00         5\n",
      "         256       1.00      1.00      1.00         5\n",
      "         257       1.00      1.00      1.00         5\n",
      "         258       1.00      0.80      0.89         5\n",
      "         259       1.00      1.00      1.00         5\n",
      "         260       1.00      1.00      1.00         5\n",
      "         261       1.00      1.00      1.00         5\n",
      "         262       1.00      0.80      0.89         5\n",
      "         263       1.00      1.00      1.00         5\n",
      "         264       1.00      1.00      1.00         5\n",
      "         265       1.00      1.00      1.00         5\n",
      "         266       1.00      1.00      1.00         5\n",
      "         267       1.00      0.80      0.89         5\n",
      "         268       1.00      1.00      1.00         5\n",
      "         269       1.00      1.00      1.00         5\n",
      "         270       1.00      0.80      0.89         5\n",
      "         271       0.83      1.00      0.91         5\n",
      "         272       0.83      1.00      0.91         5\n",
      "         273       0.83      1.00      0.91         5\n",
      "         274       0.83      1.00      0.91         5\n",
      "         275       1.00      0.80      0.89         5\n",
      "         276       1.00      1.00      1.00         5\n",
      "         277       1.00      0.80      0.89         5\n",
      "         278       0.80      0.80      0.80         5\n",
      "         279       1.00      1.00      1.00         5\n",
      "         280       1.00      1.00      1.00         5\n",
      "         281       1.00      1.00      1.00         5\n",
      "         282       0.83      1.00      0.91         5\n",
      "         283       0.83      1.00      0.91         5\n",
      "         284       1.00      1.00      1.00         5\n",
      "         285       1.00      0.60      0.75         5\n",
      "         286       1.00      1.00      1.00         5\n",
      "         287       1.00      1.00      1.00         5\n",
      "         288       1.00      1.00      1.00         5\n",
      "         289       1.00      1.00      1.00         5\n",
      "         290       1.00      1.00      1.00         5\n",
      "         291       1.00      1.00      1.00         5\n",
      "         292       1.00      1.00      1.00         5\n",
      "         293       1.00      1.00      1.00         5\n",
      "         294       0.83      1.00      0.91         5\n",
      "         295       1.00      1.00      1.00         5\n",
      "         296       0.83      1.00      0.91         5\n",
      "         297       1.00      1.00      1.00         5\n",
      "         298       1.00      1.00      1.00         5\n",
      "         299       0.83      1.00      0.91         5\n",
      "         300       0.80      0.80      0.80         5\n",
      "         301       1.00      1.00      1.00         5\n",
      "         302       1.00      0.80      0.89         5\n",
      "         303       1.00      1.00      1.00         5\n",
      "         304       1.00      1.00      1.00         5\n",
      "         305       0.83      1.00      0.91         5\n",
      "         306       0.62      1.00      0.77         5\n",
      "         307       0.83      1.00      0.91         5\n",
      "         308       1.00      1.00      1.00         5\n",
      "         309       1.00      1.00      1.00         5\n",
      "         310       1.00      1.00      1.00         5\n",
      "         311       1.00      1.00      1.00         5\n",
      "         312       1.00      1.00      1.00         5\n",
      "         313       1.00      0.80      0.89         5\n",
      "         314       1.00      1.00      1.00         5\n",
      "         315       1.00      1.00      1.00         5\n",
      "         316       1.00      1.00      1.00         5\n",
      "         317       1.00      1.00      1.00         5\n",
      "         318       1.00      1.00      1.00         5\n",
      "         319       1.00      0.80      0.89         5\n",
      "         320       0.57      0.80      0.67         5\n",
      "         321       1.00      1.00      1.00         5\n",
      "         322       1.00      0.80      0.89         5\n",
      "         323       1.00      1.00      1.00         5\n",
      "         324       1.00      1.00      1.00         5\n",
      "         325       0.83      1.00      0.91         5\n",
      "         326       1.00      1.00      1.00         5\n",
      "         327       1.00      1.00      1.00         5\n",
      "         328       0.67      0.80      0.73         5\n",
      "         329       1.00      1.00      1.00         5\n",
      "         330       1.00      0.80      0.89         5\n",
      "         331       1.00      1.00      1.00         5\n",
      "         332       1.00      1.00      1.00         5\n",
      "         333       0.80      0.80      0.80         5\n",
      "         334       1.00      1.00      1.00         5\n",
      "         335       1.00      1.00      1.00         5\n",
      "         336       1.00      1.00      1.00         5\n",
      "         337       1.00      1.00      1.00         5\n",
      "         338       1.00      0.80      0.89         5\n",
      "         339       1.00      1.00      1.00         5\n",
      "         340       1.00      1.00      1.00         5\n",
      "         341       1.00      1.00      1.00         5\n",
      "         342       1.00      1.00      1.00         5\n",
      "         343       1.00      1.00      1.00         5\n",
      "         344       1.00      1.00      1.00         5\n",
      "         345       1.00      1.00      1.00         5\n",
      "         346       1.00      1.00      1.00         5\n",
      "         347       1.00      1.00      1.00         5\n",
      "         348       0.83      1.00      0.91         5\n",
      "         349       1.00      1.00      1.00         5\n",
      "         350       1.00      1.00      1.00         5\n",
      "         351       1.00      1.00      1.00         5\n",
      "         352       1.00      0.80      0.89         5\n",
      "         353       0.83      1.00      0.91         5\n",
      "         354       1.00      1.00      1.00         5\n",
      "         355       1.00      1.00      1.00         5\n",
      "         356       1.00      1.00      1.00         5\n",
      "         357       1.00      0.40      0.57         5\n",
      "         358       0.00      0.00      0.00         5\n",
      "         359       1.00      1.00      1.00         5\n",
      "         360       1.00      1.00      1.00         5\n",
      "         361       1.00      1.00      1.00         5\n",
      "         362       1.00      1.00      1.00         5\n",
      "         363       1.00      1.00      1.00         5\n",
      "         364       0.83      1.00      0.91         5\n",
      "         365       1.00      1.00      1.00         5\n",
      "         366       1.00      0.80      0.89         5\n",
      "         367       0.83      1.00      0.91         5\n",
      "         368       1.00      1.00      1.00         5\n",
      "         369       1.00      0.80      0.89         5\n",
      "         370       1.00      1.00      1.00         5\n",
      "         371       0.80      0.80      0.80         5\n",
      "         372       1.00      1.00      1.00         5\n",
      "         373       0.50      1.00      0.67         5\n",
      "         374       1.00      1.00      1.00         5\n",
      "         375       0.83      1.00      0.91         5\n",
      "         376       1.00      1.00      1.00         5\n",
      "         377       1.00      0.80      0.89         5\n",
      "         378       1.00      1.00      1.00         5\n",
      "         379       1.00      1.00      1.00         5\n",
      "         380       1.00      1.00      1.00         5\n",
      "         381       1.00      0.60      0.75         5\n",
      "         382       1.00      1.00      1.00         5\n",
      "         383       0.80      0.80      0.80         5\n",
      "         384       0.83      1.00      0.91         5\n",
      "         385       1.00      1.00      1.00         5\n",
      "         386       0.83      1.00      0.91         5\n",
      "         387       1.00      1.00      1.00         5\n",
      "         388       1.00      1.00      1.00         5\n",
      "         389       1.00      1.00      1.00         5\n",
      "         390       1.00      0.80      0.89         5\n",
      "         391       1.00      1.00      1.00         5\n",
      "         392       1.00      1.00      1.00         5\n",
      "         393       0.83      1.00      0.91         5\n",
      "         394       1.00      1.00      1.00         5\n",
      "         395       1.00      1.00      1.00         5\n",
      "         396       1.00      1.00      1.00         5\n",
      "         397       1.00      0.40      0.57         5\n",
      "         398       1.00      1.00      1.00         5\n",
      "         399       1.00      0.80      0.89         5\n",
      "\n",
      "    accuracy                           0.95      2000\n",
      "   macro avg       0.96      0.95      0.95      2000\n",
      "weighted avg       0.96      0.95      0.95      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\ryu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, df_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d400024a",
   "metadata": {},
   "source": [
    "ทำ FINE TUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa60f4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('save_model/hyperband_modelsave0.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "212db9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_layer_trainable(conv_model):\n",
    "    for layer in conv_model.layers:\n",
    "        print(\"{0}:\\t{1}\".format(layer.trainable, layer.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "122c9581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True:\tinput_3\n",
      "False:\tconv2d_188\n",
      "False:\tbatch_normalization_190\n",
      "False:\tactivation_193\n",
      "False:\tconv2d_189\n",
      "False:\tbatch_normalization_191\n",
      "False:\tactivation_194\n",
      "False:\tconv2d_190\n",
      "False:\tbatch_normalization_192\n",
      "False:\tactivation_195\n",
      "False:\tmax_pooling2d_8\n",
      "False:\tconv2d_191\n",
      "False:\tbatch_normalization_193\n",
      "False:\tactivation_196\n",
      "False:\tconv2d_192\n",
      "False:\tbatch_normalization_194\n",
      "False:\tactivation_197\n",
      "False:\tmax_pooling2d_9\n",
      "False:\tconv2d_196\n",
      "False:\tbatch_normalization_198\n",
      "False:\tactivation_201\n",
      "False:\tconv2d_194\n",
      "False:\tconv2d_197\n",
      "False:\tbatch_normalization_196\n",
      "False:\tbatch_normalization_199\n",
      "False:\tactivation_199\n",
      "False:\tactivation_202\n",
      "False:\taverage_pooling2d_18\n",
      "False:\tconv2d_193\n",
      "False:\tconv2d_195\n",
      "False:\tconv2d_198\n",
      "False:\tconv2d_199\n",
      "False:\tbatch_normalization_195\n",
      "False:\tbatch_normalization_197\n",
      "False:\tbatch_normalization_200\n",
      "False:\tbatch_normalization_201\n",
      "False:\tactivation_198\n",
      "False:\tactivation_200\n",
      "False:\tactivation_203\n",
      "False:\tactivation_204\n",
      "False:\tmixed0\n",
      "False:\tconv2d_203\n",
      "False:\tbatch_normalization_205\n",
      "False:\tactivation_208\n",
      "False:\tconv2d_201\n",
      "False:\tconv2d_204\n",
      "False:\tbatch_normalization_203\n",
      "False:\tbatch_normalization_206\n",
      "False:\tactivation_206\n",
      "False:\tactivation_209\n",
      "False:\taverage_pooling2d_19\n",
      "False:\tconv2d_200\n",
      "False:\tconv2d_202\n",
      "False:\tconv2d_205\n",
      "False:\tconv2d_206\n",
      "False:\tbatch_normalization_202\n",
      "False:\tbatch_normalization_204\n",
      "False:\tbatch_normalization_207\n",
      "False:\tbatch_normalization_208\n",
      "False:\tactivation_205\n",
      "False:\tactivation_207\n",
      "False:\tactivation_210\n",
      "False:\tactivation_211\n",
      "False:\tmixed1\n",
      "False:\tconv2d_210\n",
      "False:\tbatch_normalization_212\n",
      "False:\tactivation_215\n",
      "False:\tconv2d_208\n",
      "False:\tconv2d_211\n",
      "False:\tbatch_normalization_210\n",
      "False:\tbatch_normalization_213\n",
      "False:\tactivation_213\n",
      "False:\tactivation_216\n",
      "False:\taverage_pooling2d_20\n",
      "False:\tconv2d_207\n",
      "False:\tconv2d_209\n",
      "False:\tconv2d_212\n",
      "False:\tconv2d_213\n",
      "False:\tbatch_normalization_209\n",
      "False:\tbatch_normalization_211\n",
      "False:\tbatch_normalization_214\n",
      "False:\tbatch_normalization_215\n",
      "False:\tactivation_212\n",
      "False:\tactivation_214\n",
      "False:\tactivation_217\n",
      "False:\tactivation_218\n",
      "False:\tmixed2\n",
      "False:\tconv2d_215\n",
      "False:\tbatch_normalization_217\n",
      "False:\tactivation_220\n",
      "False:\tconv2d_216\n",
      "False:\tbatch_normalization_218\n",
      "False:\tactivation_221\n",
      "False:\tconv2d_214\n",
      "False:\tconv2d_217\n",
      "False:\tbatch_normalization_216\n",
      "False:\tbatch_normalization_219\n",
      "False:\tactivation_219\n",
      "False:\tactivation_222\n",
      "False:\tmax_pooling2d_10\n",
      "False:\tmixed3\n",
      "False:\tconv2d_222\n",
      "False:\tbatch_normalization_224\n",
      "False:\tactivation_227\n",
      "False:\tconv2d_223\n",
      "False:\tbatch_normalization_225\n",
      "False:\tactivation_228\n",
      "False:\tconv2d_219\n",
      "False:\tconv2d_224\n",
      "False:\tbatch_normalization_221\n",
      "False:\tbatch_normalization_226\n",
      "False:\tactivation_224\n",
      "False:\tactivation_229\n",
      "False:\tconv2d_220\n",
      "False:\tconv2d_225\n",
      "False:\tbatch_normalization_222\n",
      "False:\tbatch_normalization_227\n",
      "False:\tactivation_225\n",
      "False:\tactivation_230\n",
      "False:\taverage_pooling2d_21\n",
      "False:\tconv2d_218\n",
      "False:\tconv2d_221\n",
      "False:\tconv2d_226\n",
      "False:\tconv2d_227\n",
      "False:\tbatch_normalization_220\n",
      "False:\tbatch_normalization_223\n",
      "False:\tbatch_normalization_228\n",
      "False:\tbatch_normalization_229\n",
      "False:\tactivation_223\n",
      "False:\tactivation_226\n",
      "False:\tactivation_231\n",
      "False:\tactivation_232\n",
      "False:\tmixed4\n",
      "False:\tconv2d_232\n",
      "False:\tbatch_normalization_234\n",
      "False:\tactivation_237\n",
      "False:\tconv2d_233\n",
      "False:\tbatch_normalization_235\n",
      "False:\tactivation_238\n",
      "False:\tconv2d_229\n",
      "False:\tconv2d_234\n",
      "False:\tbatch_normalization_231\n",
      "False:\tbatch_normalization_236\n",
      "False:\tactivation_234\n",
      "False:\tactivation_239\n",
      "False:\tconv2d_230\n",
      "False:\tconv2d_235\n",
      "False:\tbatch_normalization_232\n",
      "False:\tbatch_normalization_237\n",
      "False:\tactivation_235\n",
      "False:\tactivation_240\n",
      "False:\taverage_pooling2d_22\n",
      "False:\tconv2d_228\n",
      "False:\tconv2d_231\n",
      "False:\tconv2d_236\n",
      "False:\tconv2d_237\n",
      "False:\tbatch_normalization_230\n",
      "False:\tbatch_normalization_233\n",
      "False:\tbatch_normalization_238\n",
      "False:\tbatch_normalization_239\n",
      "False:\tactivation_233\n",
      "False:\tactivation_236\n",
      "False:\tactivation_241\n",
      "False:\tactivation_242\n",
      "False:\tmixed5\n",
      "False:\tconv2d_242\n",
      "False:\tbatch_normalization_244\n",
      "False:\tactivation_247\n",
      "False:\tconv2d_243\n",
      "False:\tbatch_normalization_245\n",
      "False:\tactivation_248\n",
      "False:\tconv2d_239\n",
      "False:\tconv2d_244\n",
      "False:\tbatch_normalization_241\n",
      "False:\tbatch_normalization_246\n",
      "False:\tactivation_244\n",
      "False:\tactivation_249\n",
      "False:\tconv2d_240\n",
      "False:\tconv2d_245\n",
      "False:\tbatch_normalization_242\n",
      "False:\tbatch_normalization_247\n",
      "False:\tactivation_245\n",
      "False:\tactivation_250\n",
      "False:\taverage_pooling2d_23\n",
      "False:\tconv2d_238\n",
      "False:\tconv2d_241\n",
      "False:\tconv2d_246\n",
      "False:\tconv2d_247\n",
      "False:\tbatch_normalization_240\n",
      "False:\tbatch_normalization_243\n",
      "False:\tbatch_normalization_248\n",
      "False:\tbatch_normalization_249\n",
      "False:\tactivation_243\n",
      "False:\tactivation_246\n",
      "False:\tactivation_251\n",
      "False:\tactivation_252\n",
      "False:\tmixed6\n",
      "False:\tconv2d_252\n",
      "False:\tbatch_normalization_254\n",
      "False:\tactivation_257\n",
      "False:\tconv2d_253\n",
      "False:\tbatch_normalization_255\n",
      "False:\tactivation_258\n",
      "False:\tconv2d_249\n",
      "False:\tconv2d_254\n",
      "False:\tbatch_normalization_251\n",
      "False:\tbatch_normalization_256\n",
      "False:\tactivation_254\n",
      "False:\tactivation_259\n",
      "False:\tconv2d_250\n",
      "False:\tconv2d_255\n",
      "False:\tbatch_normalization_252\n",
      "False:\tbatch_normalization_257\n",
      "False:\tactivation_255\n",
      "False:\tactivation_260\n",
      "False:\taverage_pooling2d_24\n",
      "False:\tconv2d_248\n",
      "False:\tconv2d_251\n",
      "False:\tconv2d_256\n",
      "False:\tconv2d_257\n",
      "False:\tbatch_normalization_250\n",
      "False:\tbatch_normalization_253\n",
      "False:\tbatch_normalization_258\n",
      "False:\tbatch_normalization_259\n",
      "False:\tactivation_253\n",
      "False:\tactivation_256\n",
      "False:\tactivation_261\n",
      "False:\tactivation_262\n",
      "False:\tmixed7\n",
      "False:\tconv2d_260\n",
      "False:\tbatch_normalization_262\n",
      "False:\tactivation_265\n",
      "False:\tconv2d_261\n",
      "False:\tbatch_normalization_263\n",
      "False:\tactivation_266\n",
      "False:\tconv2d_258\n",
      "False:\tconv2d_262\n",
      "False:\tbatch_normalization_260\n",
      "False:\tbatch_normalization_264\n",
      "False:\tactivation_263\n",
      "False:\tactivation_267\n",
      "False:\tconv2d_259\n",
      "False:\tconv2d_263\n",
      "False:\tbatch_normalization_261\n",
      "False:\tbatch_normalization_265\n",
      "False:\tactivation_264\n",
      "False:\tactivation_268\n",
      "False:\tmax_pooling2d_11\n",
      "False:\tmixed8\n",
      "False:\tconv2d_268\n",
      "False:\tbatch_normalization_270\n",
      "False:\tactivation_273\n",
      "False:\tconv2d_265\n",
      "False:\tconv2d_269\n",
      "False:\tbatch_normalization_267\n",
      "False:\tbatch_normalization_271\n",
      "False:\tactivation_270\n",
      "False:\tactivation_274\n",
      "False:\tconv2d_266\n",
      "False:\tconv2d_267\n",
      "False:\tconv2d_270\n",
      "False:\tconv2d_271\n",
      "False:\taverage_pooling2d_25\n",
      "False:\tconv2d_264\n",
      "False:\tbatch_normalization_268\n",
      "False:\tbatch_normalization_269\n",
      "False:\tbatch_normalization_272\n",
      "False:\tbatch_normalization_273\n",
      "False:\tconv2d_272\n",
      "False:\tbatch_normalization_266\n",
      "False:\tactivation_271\n",
      "False:\tactivation_272\n",
      "False:\tactivation_275\n",
      "False:\tactivation_276\n",
      "False:\tbatch_normalization_274\n",
      "False:\tactivation_269\n",
      "False:\tmixed9_0\n",
      "False:\tconcatenate_4\n",
      "False:\tactivation_277\n",
      "False:\tmixed9\n",
      "False:\tconv2d_277\n",
      "False:\tbatch_normalization_279\n",
      "False:\tactivation_282\n",
      "False:\tconv2d_274\n",
      "False:\tconv2d_278\n",
      "False:\tbatch_normalization_276\n",
      "False:\tbatch_normalization_280\n",
      "False:\tactivation_279\n",
      "False:\tactivation_283\n",
      "False:\tconv2d_275\n",
      "False:\tconv2d_276\n",
      "False:\tconv2d_279\n",
      "False:\tconv2d_280\n",
      "False:\taverage_pooling2d_26\n",
      "False:\tconv2d_273\n",
      "False:\tbatch_normalization_277\n",
      "False:\tbatch_normalization_278\n",
      "False:\tbatch_normalization_281\n",
      "False:\tbatch_normalization_282\n",
      "False:\tconv2d_281\n",
      "False:\tbatch_normalization_275\n",
      "False:\tactivation_280\n",
      "False:\tactivation_281\n",
      "False:\tactivation_284\n",
      "False:\tactivation_285\n",
      "False:\tbatch_normalization_283\n",
      "False:\tactivation_278\n",
      "False:\tmixed9_1\n",
      "False:\tconcatenate_5\n",
      "False:\tactivation_286\n",
      "False:\tmixed10\n",
      "True:\tflatten_2\n",
      "True:\tdense_7\n",
      "True:\tdropout_5\n",
      "True:\tbatch_normalization_284\n",
      "True:\tactivation_287\n",
      "True:\tdense_8\n",
      "True:\tdropout_6\n",
      "True:\tactivation_288\n",
      "True:\tdense_9\n"
     ]
    }
   ],
   "source": [
    "print_layer_trainable(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f75d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:]:\n",
    "    if ('batch_normalization' in layer.name):\n",
    "        trainable = False\n",
    "    else:\n",
    "        trainable = True\n",
    "    if ('batch_normalization_284' in layer.name):\n",
    "        trainable = True\n",
    "    layer.trainable = trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ace73f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True:\tinput_3\n",
      "True:\tconv2d_188\n",
      "False:\tbatch_normalization_190\n",
      "True:\tactivation_193\n",
      "True:\tconv2d_189\n",
      "False:\tbatch_normalization_191\n",
      "True:\tactivation_194\n",
      "True:\tconv2d_190\n",
      "False:\tbatch_normalization_192\n",
      "True:\tactivation_195\n",
      "True:\tmax_pooling2d_8\n",
      "True:\tconv2d_191\n",
      "False:\tbatch_normalization_193\n",
      "True:\tactivation_196\n",
      "True:\tconv2d_192\n",
      "False:\tbatch_normalization_194\n",
      "True:\tactivation_197\n",
      "True:\tmax_pooling2d_9\n",
      "True:\tconv2d_196\n",
      "False:\tbatch_normalization_198\n",
      "True:\tactivation_201\n",
      "True:\tconv2d_194\n",
      "True:\tconv2d_197\n",
      "False:\tbatch_normalization_196\n",
      "False:\tbatch_normalization_199\n",
      "True:\tactivation_199\n",
      "True:\tactivation_202\n",
      "True:\taverage_pooling2d_18\n",
      "True:\tconv2d_193\n",
      "True:\tconv2d_195\n",
      "True:\tconv2d_198\n",
      "True:\tconv2d_199\n",
      "False:\tbatch_normalization_195\n",
      "False:\tbatch_normalization_197\n",
      "False:\tbatch_normalization_200\n",
      "False:\tbatch_normalization_201\n",
      "True:\tactivation_198\n",
      "True:\tactivation_200\n",
      "True:\tactivation_203\n",
      "True:\tactivation_204\n",
      "True:\tmixed0\n",
      "True:\tconv2d_203\n",
      "False:\tbatch_normalization_205\n",
      "True:\tactivation_208\n",
      "True:\tconv2d_201\n",
      "True:\tconv2d_204\n",
      "False:\tbatch_normalization_203\n",
      "False:\tbatch_normalization_206\n",
      "True:\tactivation_206\n",
      "True:\tactivation_209\n",
      "True:\taverage_pooling2d_19\n",
      "True:\tconv2d_200\n",
      "True:\tconv2d_202\n",
      "True:\tconv2d_205\n",
      "True:\tconv2d_206\n",
      "False:\tbatch_normalization_202\n",
      "False:\tbatch_normalization_204\n",
      "False:\tbatch_normalization_207\n",
      "False:\tbatch_normalization_208\n",
      "True:\tactivation_205\n",
      "True:\tactivation_207\n",
      "True:\tactivation_210\n",
      "True:\tactivation_211\n",
      "True:\tmixed1\n",
      "True:\tconv2d_210\n",
      "False:\tbatch_normalization_212\n",
      "True:\tactivation_215\n",
      "True:\tconv2d_208\n",
      "True:\tconv2d_211\n",
      "False:\tbatch_normalization_210\n",
      "False:\tbatch_normalization_213\n",
      "True:\tactivation_213\n",
      "True:\tactivation_216\n",
      "True:\taverage_pooling2d_20\n",
      "True:\tconv2d_207\n",
      "True:\tconv2d_209\n",
      "True:\tconv2d_212\n",
      "True:\tconv2d_213\n",
      "False:\tbatch_normalization_209\n",
      "False:\tbatch_normalization_211\n",
      "False:\tbatch_normalization_214\n",
      "False:\tbatch_normalization_215\n",
      "True:\tactivation_212\n",
      "True:\tactivation_214\n",
      "True:\tactivation_217\n",
      "True:\tactivation_218\n",
      "True:\tmixed2\n",
      "True:\tconv2d_215\n",
      "False:\tbatch_normalization_217\n",
      "True:\tactivation_220\n",
      "True:\tconv2d_216\n",
      "False:\tbatch_normalization_218\n",
      "True:\tactivation_221\n",
      "True:\tconv2d_214\n",
      "True:\tconv2d_217\n",
      "False:\tbatch_normalization_216\n",
      "False:\tbatch_normalization_219\n",
      "True:\tactivation_219\n",
      "True:\tactivation_222\n",
      "True:\tmax_pooling2d_10\n",
      "True:\tmixed3\n",
      "True:\tconv2d_222\n",
      "False:\tbatch_normalization_224\n",
      "True:\tactivation_227\n",
      "True:\tconv2d_223\n",
      "False:\tbatch_normalization_225\n",
      "True:\tactivation_228\n",
      "True:\tconv2d_219\n",
      "True:\tconv2d_224\n",
      "False:\tbatch_normalization_221\n",
      "False:\tbatch_normalization_226\n",
      "True:\tactivation_224\n",
      "True:\tactivation_229\n",
      "True:\tconv2d_220\n",
      "True:\tconv2d_225\n",
      "False:\tbatch_normalization_222\n",
      "False:\tbatch_normalization_227\n",
      "True:\tactivation_225\n",
      "True:\tactivation_230\n",
      "True:\taverage_pooling2d_21\n",
      "True:\tconv2d_218\n",
      "True:\tconv2d_221\n",
      "True:\tconv2d_226\n",
      "True:\tconv2d_227\n",
      "False:\tbatch_normalization_220\n",
      "False:\tbatch_normalization_223\n",
      "False:\tbatch_normalization_228\n",
      "False:\tbatch_normalization_229\n",
      "True:\tactivation_223\n",
      "True:\tactivation_226\n",
      "True:\tactivation_231\n",
      "True:\tactivation_232\n",
      "True:\tmixed4\n",
      "True:\tconv2d_232\n",
      "False:\tbatch_normalization_234\n",
      "True:\tactivation_237\n",
      "True:\tconv2d_233\n",
      "False:\tbatch_normalization_235\n",
      "True:\tactivation_238\n",
      "True:\tconv2d_229\n",
      "True:\tconv2d_234\n",
      "False:\tbatch_normalization_231\n",
      "False:\tbatch_normalization_236\n",
      "True:\tactivation_234\n",
      "True:\tactivation_239\n",
      "True:\tconv2d_230\n",
      "True:\tconv2d_235\n",
      "False:\tbatch_normalization_232\n",
      "False:\tbatch_normalization_237\n",
      "True:\tactivation_235\n",
      "True:\tactivation_240\n",
      "True:\taverage_pooling2d_22\n",
      "True:\tconv2d_228\n",
      "True:\tconv2d_231\n",
      "True:\tconv2d_236\n",
      "True:\tconv2d_237\n",
      "False:\tbatch_normalization_230\n",
      "False:\tbatch_normalization_233\n",
      "False:\tbatch_normalization_238\n",
      "False:\tbatch_normalization_239\n",
      "True:\tactivation_233\n",
      "True:\tactivation_236\n",
      "True:\tactivation_241\n",
      "True:\tactivation_242\n",
      "True:\tmixed5\n",
      "True:\tconv2d_242\n",
      "False:\tbatch_normalization_244\n",
      "True:\tactivation_247\n",
      "True:\tconv2d_243\n",
      "False:\tbatch_normalization_245\n",
      "True:\tactivation_248\n",
      "True:\tconv2d_239\n",
      "True:\tconv2d_244\n",
      "False:\tbatch_normalization_241\n",
      "False:\tbatch_normalization_246\n",
      "True:\tactivation_244\n",
      "True:\tactivation_249\n",
      "True:\tconv2d_240\n",
      "True:\tconv2d_245\n",
      "False:\tbatch_normalization_242\n",
      "False:\tbatch_normalization_247\n",
      "True:\tactivation_245\n",
      "True:\tactivation_250\n",
      "True:\taverage_pooling2d_23\n",
      "True:\tconv2d_238\n",
      "True:\tconv2d_241\n",
      "True:\tconv2d_246\n",
      "True:\tconv2d_247\n",
      "False:\tbatch_normalization_240\n",
      "False:\tbatch_normalization_243\n",
      "False:\tbatch_normalization_248\n",
      "False:\tbatch_normalization_249\n",
      "True:\tactivation_243\n",
      "True:\tactivation_246\n",
      "True:\tactivation_251\n",
      "True:\tactivation_252\n",
      "True:\tmixed6\n",
      "True:\tconv2d_252\n",
      "False:\tbatch_normalization_254\n",
      "True:\tactivation_257\n",
      "True:\tconv2d_253\n",
      "False:\tbatch_normalization_255\n",
      "True:\tactivation_258\n",
      "True:\tconv2d_249\n",
      "True:\tconv2d_254\n",
      "False:\tbatch_normalization_251\n",
      "False:\tbatch_normalization_256\n",
      "True:\tactivation_254\n",
      "True:\tactivation_259\n",
      "True:\tconv2d_250\n",
      "True:\tconv2d_255\n",
      "False:\tbatch_normalization_252\n",
      "False:\tbatch_normalization_257\n",
      "True:\tactivation_255\n",
      "True:\tactivation_260\n",
      "True:\taverage_pooling2d_24\n",
      "True:\tconv2d_248\n",
      "True:\tconv2d_251\n",
      "True:\tconv2d_256\n",
      "True:\tconv2d_257\n",
      "False:\tbatch_normalization_250\n",
      "False:\tbatch_normalization_253\n",
      "False:\tbatch_normalization_258\n",
      "False:\tbatch_normalization_259\n",
      "True:\tactivation_253\n",
      "True:\tactivation_256\n",
      "True:\tactivation_261\n",
      "True:\tactivation_262\n",
      "True:\tmixed7\n",
      "True:\tconv2d_260\n",
      "False:\tbatch_normalization_262\n",
      "True:\tactivation_265\n",
      "True:\tconv2d_261\n",
      "False:\tbatch_normalization_263\n",
      "True:\tactivation_266\n",
      "True:\tconv2d_258\n",
      "True:\tconv2d_262\n",
      "False:\tbatch_normalization_260\n",
      "False:\tbatch_normalization_264\n",
      "True:\tactivation_263\n",
      "True:\tactivation_267\n",
      "True:\tconv2d_259\n",
      "True:\tconv2d_263\n",
      "False:\tbatch_normalization_261\n",
      "False:\tbatch_normalization_265\n",
      "True:\tactivation_264\n",
      "True:\tactivation_268\n",
      "True:\tmax_pooling2d_11\n",
      "True:\tmixed8\n",
      "True:\tconv2d_268\n",
      "False:\tbatch_normalization_270\n",
      "True:\tactivation_273\n",
      "True:\tconv2d_265\n",
      "True:\tconv2d_269\n",
      "False:\tbatch_normalization_267\n",
      "False:\tbatch_normalization_271\n",
      "True:\tactivation_270\n",
      "True:\tactivation_274\n",
      "True:\tconv2d_266\n",
      "True:\tconv2d_267\n",
      "True:\tconv2d_270\n",
      "True:\tconv2d_271\n",
      "True:\taverage_pooling2d_25\n",
      "True:\tconv2d_264\n",
      "False:\tbatch_normalization_268\n",
      "False:\tbatch_normalization_269\n",
      "False:\tbatch_normalization_272\n",
      "False:\tbatch_normalization_273\n",
      "True:\tconv2d_272\n",
      "False:\tbatch_normalization_266\n",
      "True:\tactivation_271\n",
      "True:\tactivation_272\n",
      "True:\tactivation_275\n",
      "True:\tactivation_276\n",
      "False:\tbatch_normalization_274\n",
      "True:\tactivation_269\n",
      "True:\tmixed9_0\n",
      "True:\tconcatenate_4\n",
      "True:\tactivation_277\n",
      "True:\tmixed9\n",
      "True:\tconv2d_277\n",
      "False:\tbatch_normalization_279\n",
      "True:\tactivation_282\n",
      "True:\tconv2d_274\n",
      "True:\tconv2d_278\n",
      "False:\tbatch_normalization_276\n",
      "False:\tbatch_normalization_280\n",
      "True:\tactivation_279\n",
      "True:\tactivation_283\n",
      "True:\tconv2d_275\n",
      "True:\tconv2d_276\n",
      "True:\tconv2d_279\n",
      "True:\tconv2d_280\n",
      "True:\taverage_pooling2d_26\n",
      "True:\tconv2d_273\n",
      "False:\tbatch_normalization_277\n",
      "False:\tbatch_normalization_278\n",
      "False:\tbatch_normalization_281\n",
      "False:\tbatch_normalization_282\n",
      "True:\tconv2d_281\n",
      "False:\tbatch_normalization_275\n",
      "True:\tactivation_280\n",
      "True:\tactivation_281\n",
      "True:\tactivation_284\n",
      "True:\tactivation_285\n",
      "False:\tbatch_normalization_283\n",
      "True:\tactivation_278\n",
      "True:\tmixed9_1\n",
      "True:\tconcatenate_5\n",
      "True:\tactivation_286\n",
      "True:\tmixed10\n",
      "True:\tflatten_2\n",
      "True:\tdense_7\n",
      "True:\tdropout_5\n",
      "True:\tbatch_normalization_284\n",
      "True:\tactivation_287\n",
      "True:\tdense_8\n",
      "True:\tdropout_6\n",
      "True:\tactivation_288\n",
      "True:\tdense_9\n"
     ]
    }
   ],
   "source": [
    "print_layer_trainable(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64392fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate = 0.00001),loss=\"kullback_leibler_divergence\",metrics = \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeb63710",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "checkpoint_filepath = 'fineTuneCheckpoint'\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dded496b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryu\\AppData\\Local\\Temp\\ipykernel_13264\\2130728263.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  fineTuneHistory = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.4245 - accuracy: 0.8786\n",
      "Epoch 1: val_accuracy improved from -inf to 0.95817, saving model to fineTuneCheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 509s 544ms/step - loss: 0.4245 - accuracy: 0.8786 - val_loss: 0.1413 - val_accuracy: 0.9582\n",
      "Epoch 2/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.3480 - accuracy: 0.8990\n",
      "Epoch 2: val_accuracy improved from 0.95817 to 0.96321, saving model to fineTuneCheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 425s 465ms/step - loss: 0.3480 - accuracy: 0.8990 - val_loss: 0.1181 - val_accuracy: 0.9632\n",
      "Epoch 3/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.3036 - accuracy: 0.9112\n",
      "Epoch 3: val_accuracy improved from 0.96321 to 0.96623, saving model to fineTuneCheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 430s 471ms/step - loss: 0.3036 - accuracy: 0.9112 - val_loss: 0.1125 - val_accuracy: 0.9662\n",
      "Epoch 4/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.2723 - accuracy: 0.9204\n",
      "Epoch 4: val_accuracy improved from 0.96623 to 0.96673, saving model to fineTuneCheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 437s 479ms/step - loss: 0.2723 - accuracy: 0.9204 - val_loss: 0.0994 - val_accuracy: 0.9667\n",
      "Epoch 5/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.2472 - accuracy: 0.9278\n",
      "Epoch 5: val_accuracy improved from 0.96673 to 0.97177, saving model to fineTuneCheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 430s 471ms/step - loss: 0.2472 - accuracy: 0.9278 - val_loss: 0.0951 - val_accuracy: 0.9718\n",
      "Epoch 6/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.2288 - accuracy: 0.9323\n",
      "Epoch 6: val_accuracy did not improve from 0.97177\n",
      "912/912 [==============================] - 407s 445ms/step - loss: 0.2288 - accuracy: 0.9323 - val_loss: 0.0943 - val_accuracy: 0.9713\n",
      "Epoch 7/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.2120 - accuracy: 0.9373\n",
      "Epoch 7: val_accuracy improved from 0.97177 to 0.97228, saving model to fineTuneCheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 414s 453ms/step - loss: 0.2120 - accuracy: 0.9373 - val_loss: 0.0894 - val_accuracy: 0.9723\n",
      "Epoch 8/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.1948 - accuracy: 0.9430\n",
      "Epoch 8: val_accuracy improved from 0.97228 to 0.97631, saving model to fineTuneCheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 413s 452ms/step - loss: 0.1948 - accuracy: 0.9430 - val_loss: 0.0880 - val_accuracy: 0.9763\n",
      "Epoch 9/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.1797 - accuracy: 0.9470\n",
      "Epoch 9: val_accuracy did not improve from 0.97631\n",
      "912/912 [==============================] - 390s 428ms/step - loss: 0.1797 - accuracy: 0.9470 - val_loss: 0.0907 - val_accuracy: 0.9728\n",
      "Epoch 10/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.1723 - accuracy: 0.9490\n",
      "Epoch 10: val_accuracy did not improve from 0.97631\n",
      "912/912 [==============================] - 390s 427ms/step - loss: 0.1723 - accuracy: 0.9490 - val_loss: 0.0837 - val_accuracy: 0.9733\n",
      "Epoch 11/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.1629 - accuracy: 0.9506\n",
      "Epoch 11: val_accuracy did not improve from 0.97631\n",
      "912/912 [==============================] - 390s 427ms/step - loss: 0.1629 - accuracy: 0.9506 - val_loss: 0.0895 - val_accuracy: 0.9718\n",
      "Epoch 12/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.1502 - accuracy: 0.9548\n",
      "Epoch 12: val_accuracy did not improve from 0.97631\n",
      "912/912 [==============================] - 391s 428ms/step - loss: 0.1502 - accuracy: 0.9548 - val_loss: 0.0853 - val_accuracy: 0.9763\n",
      "Epoch 13/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.1430 - accuracy: 0.9577\n",
      "Epoch 13: val_accuracy improved from 0.97631 to 0.97984, saving model to fineTuneCheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 409s 448ms/step - loss: 0.1430 - accuracy: 0.9577 - val_loss: 0.0782 - val_accuracy: 0.9798\n",
      "Epoch 14/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.1349 - accuracy: 0.9601\n",
      "Epoch 14: val_accuracy improved from 0.97984 to 0.98135, saving model to fineTuneCheckpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: fineTuneCheckpoint\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912/912 [==============================] - 411s 450ms/step - loss: 0.1349 - accuracy: 0.9601 - val_loss: 0.0731 - val_accuracy: 0.9814\n",
      "Epoch 15/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9626\n",
      "Epoch 15: val_accuracy did not improve from 0.98135\n",
      "912/912 [==============================] - 392s 429ms/step - loss: 0.1251 - accuracy: 0.9626 - val_loss: 0.0740 - val_accuracy: 0.9778\n",
      "Epoch 16/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.1222 - accuracy: 0.9637\n",
      "Epoch 16: val_accuracy did not improve from 0.98135\n",
      "912/912 [==============================] - 391s 429ms/step - loss: 0.1222 - accuracy: 0.9637 - val_loss: 0.0739 - val_accuracy: 0.9814\n",
      "Epoch 17/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.1119 - accuracy: 0.9669\n",
      "Epoch 17: val_accuracy did not improve from 0.98135\n",
      "912/912 [==============================] - 392s 429ms/step - loss: 0.1119 - accuracy: 0.9669 - val_loss: 0.0854 - val_accuracy: 0.9758\n",
      "Epoch 18/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9676\n",
      "Epoch 18: val_accuracy did not improve from 0.98135\n",
      "912/912 [==============================] - 408s 447ms/step - loss: 0.1087 - accuracy: 0.9676 - val_loss: 0.0791 - val_accuracy: 0.9803\n",
      "Epoch 19/32\n",
      "912/912 [==============================] - ETA: 0s - loss: 0.1032 - accuracy: 0.9693\n",
      "Epoch 19: val_accuracy did not improve from 0.98135\n",
      "912/912 [==============================] - 413s 453ms/step - loss: 0.1032 - accuracy: 0.9693 - val_loss: 0.0773 - val_accuracy: 0.9763\n"
     ]
    }
   ],
   "source": [
    "fineTuneHistory = model.fit_generator(\n",
    "    generator=train_generator,\n",
    "    steps_per_epoch= train_generator.n//train_generator.batch_size,\n",
    "    validation_data = val_generator,\n",
    "    validation_steps = val_generator.n//val_generator.batch_size,\n",
    "    epochs=EP,\n",
    "    callbacks = [model_checkpoint_callback,stop_early]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcd428c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('save_model/finetune_models.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e50f9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c525859030>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Performance Visualization\n",
    "# View Accuracy (Training, Validation)\n",
    "plt.plot(fineTuneHistory.history[\"accuracy\"], label=\"Train_acc\")\n",
    "plt.plot(fineTuneHistory.history[\"val_accuracy\"], label=\"Validate_acc\")\n",
    "plt.title('Training Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_acc', 'validate_acc'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c84cea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c33480c340>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(fineTuneHistory.history['loss'], label=\"Train_loss\")\n",
    "plt.plot(fineTuneHistory.history['val_loss'], label=\"Validate_loss\")\n",
    "plt.title('Training Accuracy')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train_loss', 'validate_loss'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "293452ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0, ..., 399, 399, 399])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = test_generator.classes\n",
    "y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd684f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryu\\AppData\\Local\\Temp\\ipykernel_4172\\4136271206.py:3: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  pred=model.predict_generator(test_generator)\n"
     ]
    }
   ],
   "source": [
    "test_generator.reset()\n",
    "pred_prob = []\n",
    "pred=model.predict_generator(test_generator)\n",
    "for i in range(len(y_true)):\n",
    "    pred_prob.append(np.array(pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "667a7855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.759474e-01</td>\n",
       "      <td>2.287649e-08</td>\n",
       "      <td>2.048244e-11</td>\n",
       "      <td>7.197875e-12</td>\n",
       "      <td>2.074419e-09</td>\n",
       "      <td>2.125358e-06</td>\n",
       "      <td>2.466767e-13</td>\n",
       "      <td>5.231878e-12</td>\n",
       "      <td>2.532135e-05</td>\n",
       "      <td>5.034707e-14</td>\n",
       "      <td>...</td>\n",
       "      <td>1.258771e-07</td>\n",
       "      <td>2.583779e-13</td>\n",
       "      <td>1.993724e-13</td>\n",
       "      <td>1.712492e-11</td>\n",
       "      <td>2.513636e-12</td>\n",
       "      <td>1.116494e-09</td>\n",
       "      <td>6.787831e-11</td>\n",
       "      <td>3.990897e-05</td>\n",
       "      <td>1.148783e-07</td>\n",
       "      <td>9.916527e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.660215e-01</td>\n",
       "      <td>5.086005e-07</td>\n",
       "      <td>1.328126e-11</td>\n",
       "      <td>2.551760e-12</td>\n",
       "      <td>1.618548e-07</td>\n",
       "      <td>4.499794e-03</td>\n",
       "      <td>2.149968e-10</td>\n",
       "      <td>2.633288e-09</td>\n",
       "      <td>2.598255e-05</td>\n",
       "      <td>4.957818e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>1.229005e-06</td>\n",
       "      <td>5.712876e-11</td>\n",
       "      <td>4.085169e-11</td>\n",
       "      <td>1.117733e-08</td>\n",
       "      <td>8.890768e-12</td>\n",
       "      <td>4.647363e-07</td>\n",
       "      <td>2.137625e-09</td>\n",
       "      <td>9.871848e-03</td>\n",
       "      <td>2.823319e-08</td>\n",
       "      <td>5.229154e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.903231e-01</td>\n",
       "      <td>7.519188e-10</td>\n",
       "      <td>4.249614e-12</td>\n",
       "      <td>1.258051e-12</td>\n",
       "      <td>4.564156e-08</td>\n",
       "      <td>1.713650e-07</td>\n",
       "      <td>1.217648e-13</td>\n",
       "      <td>2.578643e-11</td>\n",
       "      <td>8.131024e-07</td>\n",
       "      <td>1.013344e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>8.216742e-08</td>\n",
       "      <td>3.388941e-12</td>\n",
       "      <td>7.799824e-12</td>\n",
       "      <td>1.313311e-09</td>\n",
       "      <td>8.177833e-14</td>\n",
       "      <td>4.828824e-09</td>\n",
       "      <td>3.937140e-11</td>\n",
       "      <td>2.000886e-03</td>\n",
       "      <td>2.977247e-08</td>\n",
       "      <td>1.364776e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.434240e-01</td>\n",
       "      <td>8.144232e-07</td>\n",
       "      <td>2.441050e-11</td>\n",
       "      <td>2.121150e-13</td>\n",
       "      <td>2.072167e-07</td>\n",
       "      <td>5.456949e-07</td>\n",
       "      <td>2.969616e-12</td>\n",
       "      <td>6.447322e-12</td>\n",
       "      <td>1.535430e-06</td>\n",
       "      <td>7.348647e-13</td>\n",
       "      <td>...</td>\n",
       "      <td>8.330822e-10</td>\n",
       "      <td>3.630929e-11</td>\n",
       "      <td>3.280490e-11</td>\n",
       "      <td>1.292806e-09</td>\n",
       "      <td>1.412996e-09</td>\n",
       "      <td>2.372993e-10</td>\n",
       "      <td>3.730156e-09</td>\n",
       "      <td>9.137938e-09</td>\n",
       "      <td>1.545291e-10</td>\n",
       "      <td>1.251449e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.961885e-01</td>\n",
       "      <td>3.885694e-11</td>\n",
       "      <td>2.336257e-12</td>\n",
       "      <td>3.165989e-15</td>\n",
       "      <td>6.573500e-12</td>\n",
       "      <td>2.423764e-07</td>\n",
       "      <td>2.316113e-13</td>\n",
       "      <td>2.794543e-13</td>\n",
       "      <td>5.923185e-07</td>\n",
       "      <td>9.985326e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.092788e-09</td>\n",
       "      <td>2.259918e-14</td>\n",
       "      <td>4.136301e-14</td>\n",
       "      <td>8.915090e-12</td>\n",
       "      <td>1.020654e-14</td>\n",
       "      <td>6.453323e-11</td>\n",
       "      <td>8.935382e-12</td>\n",
       "      <td>2.947901e-06</td>\n",
       "      <td>3.007305e-13</td>\n",
       "      <td>2.521503e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>4.971870e-15</td>\n",
       "      <td>2.636415e-14</td>\n",
       "      <td>2.334503e-12</td>\n",
       "      <td>1.318370e-14</td>\n",
       "      <td>8.094873e-14</td>\n",
       "      <td>1.230805e-11</td>\n",
       "      <td>6.567493e-14</td>\n",
       "      <td>3.538883e-15</td>\n",
       "      <td>1.545593e-15</td>\n",
       "      <td>3.319994e-18</td>\n",
       "      <td>...</td>\n",
       "      <td>2.554729e-12</td>\n",
       "      <td>6.325013e-13</td>\n",
       "      <td>1.886188e-15</td>\n",
       "      <td>2.318592e-14</td>\n",
       "      <td>4.469938e-13</td>\n",
       "      <td>7.068852e-11</td>\n",
       "      <td>3.047171e-16</td>\n",
       "      <td>3.150844e-12</td>\n",
       "      <td>1.051368e-09</td>\n",
       "      <td>9.999981e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>2.415411e-14</td>\n",
       "      <td>3.270896e-13</td>\n",
       "      <td>1.623108e-12</td>\n",
       "      <td>4.449223e-15</td>\n",
       "      <td>3.248947e-13</td>\n",
       "      <td>1.896068e-11</td>\n",
       "      <td>6.722306e-13</td>\n",
       "      <td>1.133011e-14</td>\n",
       "      <td>7.677404e-14</td>\n",
       "      <td>3.386209e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.091080e-12</td>\n",
       "      <td>1.749668e-11</td>\n",
       "      <td>8.536413e-15</td>\n",
       "      <td>1.678108e-13</td>\n",
       "      <td>2.074187e-14</td>\n",
       "      <td>5.999234e-12</td>\n",
       "      <td>2.359460e-16</td>\n",
       "      <td>3.567797e-11</td>\n",
       "      <td>3.505097e-10</td>\n",
       "      <td>9.999851e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>1.885593e-13</td>\n",
       "      <td>3.188021e-13</td>\n",
       "      <td>2.336825e-12</td>\n",
       "      <td>5.600954e-15</td>\n",
       "      <td>2.027676e-12</td>\n",
       "      <td>1.710647e-11</td>\n",
       "      <td>7.581640e-13</td>\n",
       "      <td>1.917997e-15</td>\n",
       "      <td>1.863515e-13</td>\n",
       "      <td>2.781773e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>1.564603e-12</td>\n",
       "      <td>1.577350e-11</td>\n",
       "      <td>2.784481e-15</td>\n",
       "      <td>5.677650e-13</td>\n",
       "      <td>8.899135e-14</td>\n",
       "      <td>7.771337e-11</td>\n",
       "      <td>6.807129e-16</td>\n",
       "      <td>5.039593e-11</td>\n",
       "      <td>2.176206e-10</td>\n",
       "      <td>9.999946e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>3.434360e-10</td>\n",
       "      <td>6.267789e-10</td>\n",
       "      <td>3.361144e-10</td>\n",
       "      <td>3.212823e-13</td>\n",
       "      <td>1.368660e-10</td>\n",
       "      <td>3.999708e-06</td>\n",
       "      <td>3.077099e-08</td>\n",
       "      <td>4.150964e-11</td>\n",
       "      <td>3.878452e-09</td>\n",
       "      <td>1.927165e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>2.432449e-09</td>\n",
       "      <td>7.967545e-09</td>\n",
       "      <td>1.076287e-11</td>\n",
       "      <td>1.171310e-12</td>\n",
       "      <td>2.552241e-08</td>\n",
       "      <td>5.281019e-10</td>\n",
       "      <td>8.932174e-11</td>\n",
       "      <td>8.934495e-10</td>\n",
       "      <td>2.390511e-10</td>\n",
       "      <td>9.019634e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>1.740650e-16</td>\n",
       "      <td>5.000063e-14</td>\n",
       "      <td>1.688460e-13</td>\n",
       "      <td>1.382588e-17</td>\n",
       "      <td>5.219433e-14</td>\n",
       "      <td>2.033702e-13</td>\n",
       "      <td>1.742171e-13</td>\n",
       "      <td>1.200939e-16</td>\n",
       "      <td>1.666615e-15</td>\n",
       "      <td>7.721789e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>1.868908e-14</td>\n",
       "      <td>2.816967e-11</td>\n",
       "      <td>7.421357e-17</td>\n",
       "      <td>8.107206e-15</td>\n",
       "      <td>1.477917e-14</td>\n",
       "      <td>1.831650e-12</td>\n",
       "      <td>1.340234e-17</td>\n",
       "      <td>2.758813e-14</td>\n",
       "      <td>2.633521e-11</td>\n",
       "      <td>9.999988e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 400 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0             1             2             3             4    \\\n",
       "0     9.759474e-01  2.287649e-08  2.048244e-11  7.197875e-12  2.074419e-09   \n",
       "1     8.660215e-01  5.086005e-07  1.328126e-11  2.551760e-12  1.618548e-07   \n",
       "2     9.903231e-01  7.519188e-10  4.249614e-12  1.258051e-12  4.564156e-08   \n",
       "3     8.434240e-01  8.144232e-07  2.441050e-11  2.121150e-13  2.072167e-07   \n",
       "4     9.961885e-01  3.885694e-11  2.336257e-12  3.165989e-15  6.573500e-12   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1995  4.971870e-15  2.636415e-14  2.334503e-12  1.318370e-14  8.094873e-14   \n",
       "1996  2.415411e-14  3.270896e-13  1.623108e-12  4.449223e-15  3.248947e-13   \n",
       "1997  1.885593e-13  3.188021e-13  2.336825e-12  5.600954e-15  2.027676e-12   \n",
       "1998  3.434360e-10  6.267789e-10  3.361144e-10  3.212823e-13  1.368660e-10   \n",
       "1999  1.740650e-16  5.000063e-14  1.688460e-13  1.382588e-17  5.219433e-14   \n",
       "\n",
       "               5             6             7             8             9    \\\n",
       "0     2.125358e-06  2.466767e-13  5.231878e-12  2.532135e-05  5.034707e-14   \n",
       "1     4.499794e-03  2.149968e-10  2.633288e-09  2.598255e-05  4.957818e-09   \n",
       "2     1.713650e-07  1.217648e-13  2.578643e-11  8.131024e-07  1.013344e-12   \n",
       "3     5.456949e-07  2.969616e-12  6.447322e-12  1.535430e-06  7.348647e-13   \n",
       "4     2.423764e-07  2.316113e-13  2.794543e-13  5.923185e-07  9.985326e-15   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1995  1.230805e-11  6.567493e-14  3.538883e-15  1.545593e-15  3.319994e-18   \n",
       "1996  1.896068e-11  6.722306e-13  1.133011e-14  7.677404e-14  3.386209e-16   \n",
       "1997  1.710647e-11  7.581640e-13  1.917997e-15  1.863515e-13  2.781773e-16   \n",
       "1998  3.999708e-06  3.077099e-08  4.150964e-11  3.878452e-09  1.927165e-15   \n",
       "1999  2.033702e-13  1.742171e-13  1.200939e-16  1.666615e-15  7.721789e-17   \n",
       "\n",
       "      ...           390           391           392           393  \\\n",
       "0     ...  1.258771e-07  2.583779e-13  1.993724e-13  1.712492e-11   \n",
       "1     ...  1.229005e-06  5.712876e-11  4.085169e-11  1.117733e-08   \n",
       "2     ...  8.216742e-08  3.388941e-12  7.799824e-12  1.313311e-09   \n",
       "3     ...  8.330822e-10  3.630929e-11  3.280490e-11  1.292806e-09   \n",
       "4     ...  1.092788e-09  2.259918e-14  4.136301e-14  8.915090e-12   \n",
       "...   ...           ...           ...           ...           ...   \n",
       "1995  ...  2.554729e-12  6.325013e-13  1.886188e-15  2.318592e-14   \n",
       "1996  ...  1.091080e-12  1.749668e-11  8.536413e-15  1.678108e-13   \n",
       "1997  ...  1.564603e-12  1.577350e-11  2.784481e-15  5.677650e-13   \n",
       "1998  ...  2.432449e-09  7.967545e-09  1.076287e-11  1.171310e-12   \n",
       "1999  ...  1.868908e-14  2.816967e-11  7.421357e-17  8.107206e-15   \n",
       "\n",
       "               394           395           396           397           398  \\\n",
       "0     2.513636e-12  1.116494e-09  6.787831e-11  3.990897e-05  1.148783e-07   \n",
       "1     8.890768e-12  4.647363e-07  2.137625e-09  9.871848e-03  2.823319e-08   \n",
       "2     8.177833e-14  4.828824e-09  3.937140e-11  2.000886e-03  2.977247e-08   \n",
       "3     1.412996e-09  2.372993e-10  3.730156e-09  9.137938e-09  1.545291e-10   \n",
       "4     1.020654e-14  6.453323e-11  8.935382e-12  2.947901e-06  3.007305e-13   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1995  4.469938e-13  7.068852e-11  3.047171e-16  3.150844e-12  1.051368e-09   \n",
       "1996  2.074187e-14  5.999234e-12  2.359460e-16  3.567797e-11  3.505097e-10   \n",
       "1997  8.899135e-14  7.771337e-11  6.807129e-16  5.039593e-11  2.176206e-10   \n",
       "1998  2.552241e-08  5.281019e-10  8.932174e-11  8.934495e-10  2.390511e-10   \n",
       "1999  1.477917e-14  1.831650e-12  1.340234e-17  2.758813e-14  2.633521e-11   \n",
       "\n",
       "               399  \n",
       "0     9.916527e-09  \n",
       "1     5.229154e-09  \n",
       "2     1.364776e-09  \n",
       "3     1.251449e-10  \n",
       "4     2.521503e-13  \n",
       "...            ...  \n",
       "1995  9.999981e-01  \n",
       "1996  9.999851e-01  \n",
       "1997  9.999946e-01  \n",
       "1998  9.019634e-01  \n",
       "1999  9.999988e-01  \n",
       "\n",
       "[2000 rows x 400 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = pd.DataFrame(pred_prob)\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb915e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Status': True, 'Message': 'H:\\\\Bird\\\\FineTuneBird.html'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_class = df_pred.idxmax(axis=1)\n",
    "cm3 = ConfusionMatrix(y_true, list(df_class))\n",
    "cm3.save_html(\"FineTuneBird\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bed452d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         5\n",
      "           1       1.00      0.80      0.89         5\n",
      "           2       1.00      1.00      1.00         5\n",
      "           3       1.00      1.00      1.00         5\n",
      "           4       1.00      1.00      1.00         5\n",
      "           5       1.00      1.00      1.00         5\n",
      "           6       1.00      1.00      1.00         5\n",
      "           7       1.00      1.00      1.00         5\n",
      "           8       1.00      1.00      1.00         5\n",
      "           9       1.00      1.00      1.00         5\n",
      "          10       1.00      1.00      1.00         5\n",
      "          11       1.00      0.80      0.89         5\n",
      "          12       1.00      1.00      1.00         5\n",
      "          13       1.00      0.80      0.89         5\n",
      "          14       1.00      1.00      1.00         5\n",
      "          15       1.00      1.00      1.00         5\n",
      "          16       1.00      1.00      1.00         5\n",
      "          17       1.00      1.00      1.00         5\n",
      "          18       1.00      0.80      0.89         5\n",
      "          19       1.00      1.00      1.00         5\n",
      "          20       1.00      0.80      0.89         5\n",
      "          21       1.00      1.00      1.00         5\n",
      "          22       0.83      1.00      0.91         5\n",
      "          23       1.00      1.00      1.00         5\n",
      "          24       1.00      0.80      0.89         5\n",
      "          25       1.00      1.00      1.00         5\n",
      "          26       1.00      1.00      1.00         5\n",
      "          27       1.00      1.00      1.00         5\n",
      "          28       1.00      1.00      1.00         5\n",
      "          29       1.00      1.00      1.00         5\n",
      "          30       1.00      1.00      1.00         5\n",
      "          31       1.00      1.00      1.00         5\n",
      "          32       1.00      1.00      1.00         5\n",
      "          33       0.62      1.00      0.77         5\n",
      "          34       1.00      0.80      0.89         5\n",
      "          35       1.00      1.00      1.00         5\n",
      "          36       1.00      1.00      1.00         5\n",
      "          37       1.00      1.00      1.00         5\n",
      "          38       1.00      1.00      1.00         5\n",
      "          39       1.00      1.00      1.00         5\n",
      "          40       1.00      1.00      1.00         5\n",
      "          41       1.00      1.00      1.00         5\n",
      "          42       1.00      1.00      1.00         5\n",
      "          43       1.00      1.00      1.00         5\n",
      "          44       1.00      1.00      1.00         5\n",
      "          45       1.00      1.00      1.00         5\n",
      "          46       1.00      1.00      1.00         5\n",
      "          47       1.00      1.00      1.00         5\n",
      "          48       1.00      1.00      1.00         5\n",
      "          49       1.00      1.00      1.00         5\n",
      "          50       1.00      1.00      1.00         5\n",
      "          51       1.00      1.00      1.00         5\n",
      "          52       1.00      1.00      1.00         5\n",
      "          53       1.00      1.00      1.00         5\n",
      "          54       1.00      1.00      1.00         5\n",
      "          55       1.00      1.00      1.00         5\n",
      "          56       1.00      1.00      1.00         5\n",
      "          57       1.00      1.00      1.00         5\n",
      "          58       1.00      1.00      1.00         5\n",
      "          59       1.00      1.00      1.00         5\n",
      "          60       1.00      0.80      0.89         5\n",
      "          61       1.00      1.00      1.00         5\n",
      "          62       1.00      1.00      1.00         5\n",
      "          63       1.00      1.00      1.00         5\n",
      "          64       1.00      0.80      0.89         5\n",
      "          65       1.00      1.00      1.00         5\n",
      "          66       1.00      1.00      1.00         5\n",
      "          67       1.00      1.00      1.00         5\n",
      "          68       1.00      1.00      1.00         5\n",
      "          69       1.00      1.00      1.00         5\n",
      "          70       1.00      1.00      1.00         5\n",
      "          71       1.00      1.00      1.00         5\n",
      "          72       1.00      1.00      1.00         5\n",
      "          73       1.00      0.80      0.89         5\n",
      "          74       1.00      1.00      1.00         5\n",
      "          75       1.00      1.00      1.00         5\n",
      "          76       1.00      1.00      1.00         5\n",
      "          77       1.00      1.00      1.00         5\n",
      "          78       1.00      1.00      1.00         5\n",
      "          79       1.00      1.00      1.00         5\n",
      "          80       1.00      1.00      1.00         5\n",
      "          81       1.00      1.00      1.00         5\n",
      "          82       1.00      1.00      1.00         5\n",
      "          83       1.00      1.00      1.00         5\n",
      "          84       1.00      1.00      1.00         5\n",
      "          85       1.00      1.00      1.00         5\n",
      "          86       1.00      1.00      1.00         5\n",
      "          87       1.00      1.00      1.00         5\n",
      "          88       1.00      1.00      1.00         5\n",
      "          89       1.00      1.00      1.00         5\n",
      "          90       1.00      1.00      1.00         5\n",
      "          91       1.00      1.00      1.00         5\n",
      "          92       1.00      1.00      1.00         5\n",
      "          93       1.00      1.00      1.00         5\n",
      "          94       0.83      1.00      0.91         5\n",
      "          95       1.00      1.00      1.00         5\n",
      "          96       1.00      1.00      1.00         5\n",
      "          97       1.00      1.00      1.00         5\n",
      "          98       1.00      1.00      1.00         5\n",
      "          99       1.00      1.00      1.00         5\n",
      "         100       1.00      1.00      1.00         5\n",
      "         101       1.00      1.00      1.00         5\n",
      "         102       1.00      1.00      1.00         5\n",
      "         103       1.00      1.00      1.00         5\n",
      "         104       1.00      1.00      1.00         5\n",
      "         105       1.00      1.00      1.00         5\n",
      "         106       1.00      1.00      1.00         5\n",
      "         107       0.83      1.00      0.91         5\n",
      "         108       1.00      1.00      1.00         5\n",
      "         109       1.00      1.00      1.00         5\n",
      "         110       1.00      1.00      1.00         5\n",
      "         111       1.00      1.00      1.00         5\n",
      "         112       0.71      1.00      0.83         5\n",
      "         113       1.00      0.80      0.89         5\n",
      "         114       1.00      1.00      1.00         5\n",
      "         115       1.00      1.00      1.00         5\n",
      "         116       1.00      1.00      1.00         5\n",
      "         117       1.00      1.00      1.00         5\n",
      "         118       1.00      1.00      1.00         5\n",
      "         119       1.00      1.00      1.00         5\n",
      "         120       1.00      1.00      1.00         5\n",
      "         121       0.83      1.00      0.91         5\n",
      "         122       1.00      1.00      1.00         5\n",
      "         123       0.83      1.00      0.91         5\n",
      "         124       1.00      1.00      1.00         5\n",
      "         125       1.00      1.00      1.00         5\n",
      "         126       1.00      1.00      1.00         5\n",
      "         127       1.00      1.00      1.00         5\n",
      "         128       1.00      1.00      1.00         5\n",
      "         129       1.00      1.00      1.00         5\n",
      "         130       1.00      1.00      1.00         5\n",
      "         131       1.00      1.00      1.00         5\n",
      "         132       1.00      1.00      1.00         5\n",
      "         133       1.00      1.00      1.00         5\n",
      "         134       1.00      1.00      1.00         5\n",
      "         135       1.00      1.00      1.00         5\n",
      "         136       1.00      1.00      1.00         5\n",
      "         137       1.00      1.00      1.00         5\n",
      "         138       1.00      1.00      1.00         5\n",
      "         139       1.00      1.00      1.00         5\n",
      "         140       1.00      0.80      0.89         5\n",
      "         141       1.00      1.00      1.00         5\n",
      "         142       1.00      1.00      1.00         5\n",
      "         143       1.00      1.00      1.00         5\n",
      "         144       1.00      1.00      1.00         5\n",
      "         145       1.00      1.00      1.00         5\n",
      "         146       1.00      1.00      1.00         5\n",
      "         147       1.00      1.00      1.00         5\n",
      "         148       1.00      1.00      1.00         5\n",
      "         149       1.00      1.00      1.00         5\n",
      "         150       1.00      1.00      1.00         5\n",
      "         151       1.00      1.00      1.00         5\n",
      "         152       1.00      1.00      1.00         5\n",
      "         153       1.00      1.00      1.00         5\n",
      "         154       1.00      1.00      1.00         5\n",
      "         155       1.00      1.00      1.00         5\n",
      "         156       1.00      1.00      1.00         5\n",
      "         157       1.00      1.00      1.00         5\n",
      "         158       1.00      1.00      1.00         5\n",
      "         159       1.00      1.00      1.00         5\n",
      "         160       1.00      1.00      1.00         5\n",
      "         161       1.00      1.00      1.00         5\n",
      "         162       1.00      1.00      1.00         5\n",
      "         163       1.00      1.00      1.00         5\n",
      "         164       1.00      1.00      1.00         5\n",
      "         165       1.00      1.00      1.00         5\n",
      "         166       1.00      0.80      0.89         5\n",
      "         167       1.00      1.00      1.00         5\n",
      "         168       1.00      1.00      1.00         5\n",
      "         169       0.83      1.00      0.91         5\n",
      "         170       0.83      1.00      0.91         5\n",
      "         171       0.83      1.00      0.91         5\n",
      "         172       1.00      1.00      1.00         5\n",
      "         173       1.00      1.00      1.00         5\n",
      "         174       1.00      1.00      1.00         5\n",
      "         175       1.00      1.00      1.00         5\n",
      "         176       1.00      1.00      1.00         5\n",
      "         177       1.00      1.00      1.00         5\n",
      "         178       1.00      1.00      1.00         5\n",
      "         179       1.00      1.00      1.00         5\n",
      "         180       1.00      1.00      1.00         5\n",
      "         181       1.00      1.00      1.00         5\n",
      "         182       1.00      1.00      1.00         5\n",
      "         183       1.00      1.00      1.00         5\n",
      "         184       1.00      1.00      1.00         5\n",
      "         185       1.00      1.00      1.00         5\n",
      "         186       1.00      1.00      1.00         5\n",
      "         187       1.00      1.00      1.00         5\n",
      "         188       1.00      1.00      1.00         5\n",
      "         189       1.00      1.00      1.00         5\n",
      "         190       1.00      1.00      1.00         5\n",
      "         191       1.00      1.00      1.00         5\n",
      "         192       1.00      1.00      1.00         5\n",
      "         193       1.00      1.00      1.00         5\n",
      "         194       1.00      1.00      1.00         5\n",
      "         195       1.00      1.00      1.00         5\n",
      "         196       1.00      0.80      0.89         5\n",
      "         197       1.00      1.00      1.00         5\n",
      "         198       1.00      1.00      1.00         5\n",
      "         199       1.00      1.00      1.00         5\n",
      "         200       1.00      1.00      1.00         5\n",
      "         201       1.00      1.00      1.00         5\n",
      "         202       1.00      1.00      1.00         5\n",
      "         203       0.83      1.00      0.91         5\n",
      "         204       1.00      1.00      1.00         5\n",
      "         205       1.00      1.00      1.00         5\n",
      "         206       1.00      1.00      1.00         5\n",
      "         207       1.00      1.00      1.00         5\n",
      "         208       1.00      1.00      1.00         5\n",
      "         209       1.00      1.00      1.00         5\n",
      "         210       1.00      1.00      1.00         5\n",
      "         211       1.00      1.00      1.00         5\n",
      "         212       1.00      1.00      1.00         5\n",
      "         213       1.00      1.00      1.00         5\n",
      "         214       1.00      0.80      0.89         5\n",
      "         215       1.00      1.00      1.00         5\n",
      "         216       1.00      1.00      1.00         5\n",
      "         217       1.00      1.00      1.00         5\n",
      "         218       1.00      1.00      1.00         5\n",
      "         219       1.00      1.00      1.00         5\n",
      "         220       1.00      1.00      1.00         5\n",
      "         221       1.00      1.00      1.00         5\n",
      "         222       1.00      1.00      1.00         5\n",
      "         223       1.00      1.00      1.00         5\n",
      "         224       1.00      1.00      1.00         5\n",
      "         225       1.00      1.00      1.00         5\n",
      "         226       0.83      1.00      0.91         5\n",
      "         227       1.00      1.00      1.00         5\n",
      "         228       1.00      1.00      1.00         5\n",
      "         229       1.00      0.60      0.75         5\n",
      "         230       1.00      1.00      1.00         5\n",
      "         231       1.00      1.00      1.00         5\n",
      "         232       1.00      1.00      1.00         5\n",
      "         233       1.00      1.00      1.00         5\n",
      "         234       1.00      1.00      1.00         5\n",
      "         235       1.00      1.00      1.00         5\n",
      "         236       1.00      0.80      0.89         5\n",
      "         237       1.00      1.00      1.00         5\n",
      "         238       1.00      1.00      1.00         5\n",
      "         239       1.00      1.00      1.00         5\n",
      "         240       1.00      1.00      1.00         5\n",
      "         241       1.00      1.00      1.00         5\n",
      "         242       1.00      1.00      1.00         5\n",
      "         243       1.00      1.00      1.00         5\n",
      "         244       1.00      1.00      1.00         5\n",
      "         245       1.00      1.00      1.00         5\n",
      "         246       0.83      1.00      0.91         5\n",
      "         247       1.00      1.00      1.00         5\n",
      "         248       1.00      1.00      1.00         5\n",
      "         249       1.00      1.00      1.00         5\n",
      "         250       1.00      1.00      1.00         5\n",
      "         251       1.00      0.80      0.89         5\n",
      "         252       1.00      1.00      1.00         5\n",
      "         253       1.00      0.80      0.89         5\n",
      "         254       1.00      1.00      1.00         5\n",
      "         255       0.71      1.00      0.83         5\n",
      "         256       1.00      1.00      1.00         5\n",
      "         257       1.00      1.00      1.00         5\n",
      "         258       1.00      1.00      1.00         5\n",
      "         259       1.00      1.00      1.00         5\n",
      "         260       1.00      1.00      1.00         5\n",
      "         261       1.00      1.00      1.00         5\n",
      "         262       1.00      1.00      1.00         5\n",
      "         263       1.00      1.00      1.00         5\n",
      "         264       1.00      1.00      1.00         5\n",
      "         265       1.00      1.00      1.00         5\n",
      "         266       1.00      1.00      1.00         5\n",
      "         267       1.00      1.00      1.00         5\n",
      "         268       1.00      1.00      1.00         5\n",
      "         269       1.00      1.00      1.00         5\n",
      "         270       1.00      1.00      1.00         5\n",
      "         271       1.00      1.00      1.00         5\n",
      "         272       1.00      1.00      1.00         5\n",
      "         273       1.00      1.00      1.00         5\n",
      "         274       0.83      1.00      0.91         5\n",
      "         275       1.00      1.00      1.00         5\n",
      "         276       1.00      1.00      1.00         5\n",
      "         277       1.00      1.00      1.00         5\n",
      "         278       1.00      1.00      1.00         5\n",
      "         279       1.00      1.00      1.00         5\n",
      "         280       1.00      1.00      1.00         5\n",
      "         281       1.00      1.00      1.00         5\n",
      "         282       1.00      1.00      1.00         5\n",
      "         283       1.00      1.00      1.00         5\n",
      "         284       1.00      1.00      1.00         5\n",
      "         285       1.00      1.00      1.00         5\n",
      "         286       1.00      1.00      1.00         5\n",
      "         287       1.00      1.00      1.00         5\n",
      "         288       1.00      1.00      1.00         5\n",
      "         289       1.00      1.00      1.00         5\n",
      "         290       1.00      1.00      1.00         5\n",
      "         291       1.00      1.00      1.00         5\n",
      "         292       1.00      1.00      1.00         5\n",
      "         293       1.00      1.00      1.00         5\n",
      "         294       1.00      1.00      1.00         5\n",
      "         295       1.00      1.00      1.00         5\n",
      "         296       0.83      1.00      0.91         5\n",
      "         297       1.00      1.00      1.00         5\n",
      "         298       1.00      1.00      1.00         5\n",
      "         299       1.00      1.00      1.00         5\n",
      "         300       1.00      1.00      1.00         5\n",
      "         301       1.00      1.00      1.00         5\n",
      "         302       1.00      1.00      1.00         5\n",
      "         303       1.00      1.00      1.00         5\n",
      "         304       1.00      1.00      1.00         5\n",
      "         305       1.00      1.00      1.00         5\n",
      "         306       1.00      1.00      1.00         5\n",
      "         307       1.00      1.00      1.00         5\n",
      "         308       1.00      1.00      1.00         5\n",
      "         309       0.83      1.00      0.91         5\n",
      "         310       1.00      1.00      1.00         5\n",
      "         311       1.00      1.00      1.00         5\n",
      "         312       1.00      1.00      1.00         5\n",
      "         313       1.00      0.80      0.89         5\n",
      "         314       1.00      1.00      1.00         5\n",
      "         315       1.00      1.00      1.00         5\n",
      "         316       1.00      1.00      1.00         5\n",
      "         317       1.00      1.00      1.00         5\n",
      "         318       1.00      1.00      1.00         5\n",
      "         319       1.00      1.00      1.00         5\n",
      "         320       1.00      1.00      1.00         5\n",
      "         321       1.00      1.00      1.00         5\n",
      "         322       1.00      1.00      1.00         5\n",
      "         323       1.00      1.00      1.00         5\n",
      "         324       1.00      1.00      1.00         5\n",
      "         325       1.00      1.00      1.00         5\n",
      "         326       1.00      1.00      1.00         5\n",
      "         327       1.00      1.00      1.00         5\n",
      "         328       0.83      1.00      0.91         5\n",
      "         329       1.00      1.00      1.00         5\n",
      "         330       1.00      1.00      1.00         5\n",
      "         331       1.00      1.00      1.00         5\n",
      "         332       1.00      1.00      1.00         5\n",
      "         333       1.00      1.00      1.00         5\n",
      "         334       1.00      1.00      1.00         5\n",
      "         335       1.00      1.00      1.00         5\n",
      "         336       1.00      1.00      1.00         5\n",
      "         337       1.00      1.00      1.00         5\n",
      "         338       1.00      1.00      1.00         5\n",
      "         339       1.00      1.00      1.00         5\n",
      "         340       1.00      1.00      1.00         5\n",
      "         341       1.00      1.00      1.00         5\n",
      "         342       1.00      1.00      1.00         5\n",
      "         343       1.00      1.00      1.00         5\n",
      "         344       1.00      1.00      1.00         5\n",
      "         345       1.00      1.00      1.00         5\n",
      "         346       1.00      1.00      1.00         5\n",
      "         347       1.00      1.00      1.00         5\n",
      "         348       1.00      1.00      1.00         5\n",
      "         349       1.00      1.00      1.00         5\n",
      "         350       1.00      1.00      1.00         5\n",
      "         351       1.00      1.00      1.00         5\n",
      "         352       1.00      1.00      1.00         5\n",
      "         353       1.00      1.00      1.00         5\n",
      "         354       1.00      1.00      1.00         5\n",
      "         355       1.00      1.00      1.00         5\n",
      "         356       1.00      1.00      1.00         5\n",
      "         357       1.00      1.00      1.00         5\n",
      "         358       1.00      0.40      0.57         5\n",
      "         359       1.00      1.00      1.00         5\n",
      "         360       1.00      1.00      1.00         5\n",
      "         361       1.00      1.00      1.00         5\n",
      "         362       1.00      1.00      1.00         5\n",
      "         363       1.00      1.00      1.00         5\n",
      "         364       1.00      1.00      1.00         5\n",
      "         365       1.00      1.00      1.00         5\n",
      "         366       1.00      1.00      1.00         5\n",
      "         367       0.83      1.00      0.91         5\n",
      "         368       0.83      1.00      0.91         5\n",
      "         369       1.00      1.00      1.00         5\n",
      "         370       1.00      1.00      1.00         5\n",
      "         371       1.00      1.00      1.00         5\n",
      "         372       1.00      1.00      1.00         5\n",
      "         373       1.00      1.00      1.00         5\n",
      "         374       1.00      1.00      1.00         5\n",
      "         375       0.83      1.00      0.91         5\n",
      "         376       1.00      1.00      1.00         5\n",
      "         377       1.00      1.00      1.00         5\n",
      "         378       1.00      1.00      1.00         5\n",
      "         379       1.00      1.00      1.00         5\n",
      "         380       1.00      1.00      1.00         5\n",
      "         381       1.00      1.00      1.00         5\n",
      "         382       1.00      1.00      1.00         5\n",
      "         383       1.00      1.00      1.00         5\n",
      "         384       1.00      1.00      1.00         5\n",
      "         385       1.00      1.00      1.00         5\n",
      "         386       1.00      1.00      1.00         5\n",
      "         387       1.00      1.00      1.00         5\n",
      "         388       1.00      1.00      1.00         5\n",
      "         389       1.00      1.00      1.00         5\n",
      "         390       1.00      0.80      0.89         5\n",
      "         391       1.00      1.00      1.00         5\n",
      "         392       1.00      1.00      1.00         5\n",
      "         393       1.00      1.00      1.00         5\n",
      "         394       1.00      1.00      1.00         5\n",
      "         395       1.00      1.00      1.00         5\n",
      "         396       1.00      1.00      1.00         5\n",
      "         397       1.00      1.00      1.00         5\n",
      "         398       1.00      1.00      1.00         5\n",
      "         399       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.99      2000\n",
      "   macro avg       0.99      0.99      0.99      2000\n",
      "weighted avg       0.99      0.99      0.99      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, df_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
